\documentclass{book} 
\usepackage{graphicx} % Required for inserting images
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{color}
\usepackage{hyperref}
\usepackage{xypic}
\usepackage{bbm}
\usepackage{dutchcal}
\hypersetup{hidelinks,
	colorlinks=true,
	allcolors=black,
	pdfstartview=Fit,
	breaklinks=true
}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert} 
\newcommand{\leftbracket}{[}
\newcommand{\rightbracket}{]}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\begin{document}
\section{Def: Tensor}
Let $M$ and $N$ be two $R$-modules. Then exists an $R$-module denoted by $M\otimes _RN$ and a bilinear mapping 
$$t:M\times N\rightarrow M\otimes_RN$$ having the following properties:
\begin{itemize}
    \item[(1)]For any $R$-module $P$ and any bilinear mapping $s:M\times N\rightarrow P$. There exists a unique linear mapping $f_s:M\otimes_RN\rightarrow P$ such that $s=f_s\circ t$
    $$\xymatrix{
        & M\times N \ar[d]^t \ar[r]^s & P \\
        & M\otimes_RN\ar[ur]_{f_s}&
    } $$
    \item[(2)] If $T,t'$ is another couple that satisfies (1) with $s\mapsto g_s$ then there exists a unique isomorphism $$T\cong M\otimes_RN$$
\end{itemize}

Let $\mathcal{F}$ be the free $R$-module generated by $M\times N$
    $$\mathcal{F}=\{\sum\limits_{finite}a_{ij}(m_i,n_i):a_{ij}\in R,m_i\in M,n_i\in N\}$$
    let $\mathcal{G}$ be the $R$-submodule generated by the elements of the following shape $m,m'\in M\quad n,n'\in N\quad \mathbcal{r}\in R$
    $$\begin{aligned}
        &(m+m',n)-(m,n)-(m',n)\\
        &(m,n+n')-(m,n)-(m,n')\\
        &(\mathbcal{r}m,n)-\mathbcal{r}(m,n)\\
        &(m,\mathbcal{r}n)-\mathbcal{r}(m,n)
    \end{aligned}$$
    $$M\otimes_RN:=\mathcal{F}/\mathcal{G}$$
\section{Def}
$$f_s(\mathcal{G}+(m,n)):=s(m,n)$$
Extend this mapping to linearity. This makes the diagram commutative. It's clearly the unique mapping
\section{Def}
The $R$-module $M\otimes_RN$ constructed above is called the tensor product of $M$ and $N$. An element of $M\otimes_RN$ is called tensor. We denote$$t(m,n):=m\otimes n$$ and any elements of this form is called pure tensor. 
\section{Remark}Pure tensors generate $M\otimes_R N$. In particular any tensor can be written as sum of pure tensors.
\section{tensor product and duality}
\subsection{product}
Let $V_1,\cdots,V_n$ be vector spaces as above. Then $$(V_1^\vee\otimes\cdots\otimes V_n^\vee)\cong(V_1\otimes\cdots\otimes V_n)^\vee$$
\subsection{duality}
Let $V$ and $W$ be vector spaces of finite dimension. Then 
$$\mathscr{L}(V,W)\cong V^\vee\otimes W^\vee$$
\section{Def}
We went to define the tensor product of linear mappings. let$M_1,M_2,N_1,N_2$ be $R$-modules and let $f_i:M_i\rightarrow N_i$ be linear mappings. Then we define $$
\begin{aligned}
    f_1\otimes f_2:M_1\otimes M_2 &\rightarrow N_1\otimes N_2\\
    m_1\otimes m_2 &\mapsto f_1(m_1)\otimes f_2(m_2)
\end{aligned}
$$
This is a linear mapping
$$\xymatrix{
    M_1\times M_2\ar[d]\ar[r]^{f_1\times f_2}& N_1\times N_2\ar[d]\\
    M_1\otimes M_2 \ar@{-->}^{f_1\otimes f_2}[r] &N_1\otimes N_2
}$$
\section{Extension of scalars}
Let $\varphi:R\rightarrow S$ be a commutative unitary ring homomorphism. Let $M$ be a $R$-module. Goal is to give to $M$ also a structure of $S$-module "conveyed by $\varphi$"

Note that $S$ has a structure of $R$-module $s\in S,\mathbcal{r}\in R$$$\mathbcal{r}s:=\varphi(\mathbcal{r})s$$
Now take thw tensor product $M\otimes_RS$. Now we give a structure of $S$-module to $M\otimes_RS$. 

Take $s\in S$$$s(\underbrace{m\otimes s'}\limits_{\in M\otimes_RS}):=m\otimes ss'$$
note that $ss'$ is a multi in $S$ and we cannot product $sm$.

Notice we've a mapping
$$\begin{aligned}
    i: M&\rightarrow M\otimes_RS\\
    m &\mapsto m\otimes s
\end{aligned}$$
Be careful, in general the mapping $i$ is NOT injective.
\section{Prop}
Let $K\subseteq L$ be a field extension and let $V$ be a $K$-vector space. Moreover let's denote $V_L=V\otimes_KL$. If $\{e_i\}_{i=1}^n$ is a basis of $V$ then $\{e_i\otimes 1\}_{i=1}^n$ is a L-basis of $V_L$.($V_L$ has the same dim of $V$)
\section{Def}
We denote
$$
\begin{aligned}
    T_p^q:=&(V^\vee)^{\otimes p}\otimes V^{\otimes q}\qquad p,q\in \mathbb{N}\\
    =&\underbrace{V^\vee\otimes\cdots\otimes V^\vee}\limits_{p\text{ times}}\otimes \underbrace{V\otimes\cdots\otimes V}\limits_{q\text{ times}}\\
\end{aligned}
$$
An element of $T_p^q(V)$ is called a tensor of type $(p,q)$ (or a mixed tensor which is $p$-covariant and $q$-contravariant)

Let's denote:
$$T(V):=\bigoplus\limits_{q\in \mathbb{N}} T_0^q(V)$$
On $T(V)$ we have following operation:
$$
\begin{aligned}
    T_0^l(V)\times T_0^q(V)&\rightarrow T_0^{l+q}(V)\\
    ((x_1\otimes\cdots\otimes x_l),(y_1\otimes\cdots\otimes y_q))&\mapsto x_1\otimes\cdots\otimes x_l\otimes y_1\otimes\cdots\otimes y_q
\end{aligned}
$$

With this operation $T(V)$ becomes a $K$-algebra. It called the tensor algebra associated to $V$

\section{Def}
The quotient algebra $$\bigwedge(V):=T(V)/\left\{\sum\limits_{i(finite)}(y_1\otimes\cdots\otimes y_{m_i})\otimes(x_i\otimes x_i)\otimes(z_1\otimes\cdots\otimes z_{n_i})\right\}$$
is a K-algebra, which called the exterior algebra of V
$$
\begin{aligned}
    \pi: &T(V) &\rightarrow \bigwedge(V)\\
    &x_1\otimes\cdots\otimes x_n &\mapsto x_1\wedge\cdots\wedge x_n
\end{aligned}$$
\section{Notation}
$$\bigwedge(V)=\bigoplus \limits_{n\in\mathbb{N}} \bigwedge^n(V)$$
$$\bigwedge^n(V):=T^n_0(V)/(W\cap T_0^n(V))$$
this is called $n$-fold exterior product
\section{Prop}
\label{VIII-46.8}
FIx a vct space V. For any alternating multi-linear mapping
$$s:\underbrace{V\times\cdots\times V}\limits_{n\text{ times}}\rightarrow W$$
when W is another vct space, there exists a unique linear mapping$$g_s:\bigwedge\limits^n(V)\rightarrow W$$
such that the following diagram commutes
$$\xymatrix{
    & V^n\ar[d]_t\ar[r]^s & W\\
    &T^n_0(V)\ar@{-->}[ur]^{f_s}\ar[d] &\\
    & \bigwedge\limits^n(V)\ar[uur]_{g_s}
}$$
\section{Prop}
Let V be a vct space of dimension n with a basis $\{e_1,\cdots,e_n\}$. Then $\bigwedge\limits^k(V)$ is a vct space with a basis given by $$\mathcal{B}=\{e_{i_1}\wedge\cdots\wedge e_{i_k}\big|\ 1\leq i_1<\cdots<i_k\leq n\}$$
In particular, $\bigwedge\limits^k(V)$ has dimension $\left(\begin{aligned}
    &n \\ &k
\end{aligned}
\right)$
\section{Def}
Let V be a vct space of dimension n, then 
$$\det(V)=\bigwedge\limits^n(V)$$
is called the determinant of V. It is a vct space of dimension $1=\binom{n}{n}$ and a basis is given by $$e_1\wedge\cdots\wedge e_n$$
when $\{e_1,\cdots,e_n\}$ is a basis of V.
\section{Def}
$$
\begin{aligned}
    g_{\widetilde{f}}=\bigwedge\limits^kf: &\bigwedge\limits^k(V) &\rightarrow &\bigwedge\limits^k(V)\\
    &v_1\wedge\cdots\wedge v_k &\mapsto & f(v_1)\wedge\cdots\wedge f(v_n)
\end{aligned}$$
\section{Def}
Let $F:V\rightarrow V$ be a linear mapping. A subspace $V_0\subseteq V$ is said to be an invariant subspace of $F$ is $F(V_0)\subseteq V_0$
\section{Def}
A linear mapping $f:V\rightarrow V$ (finite dim) is diagonalizable if the following equivalent conditions are satisfied
\begin{itemize}
    \item [1] $V$ decomposes as a direct sum of one-dimensional invariant subspace of $f$
    \item [2] There exists a basis of $V$, in which the matrix $A_f$ is diagonal.
\end{itemize}
\section{Def}
V a vector space over K $dim(V)=n,f\in \mathscr{L}(V;V)$ let $A_f$ be an associated matrix (in any basis) the mapping
$$\begin{aligned}
    P: &K &\rightarrow& K\\&t&\mapsto& \det(tI_n-A_f)
\end{aligned}$$
This is a polynomial in $K[t]$ (with degree $n$)
\section{Def}
Let $a_0+a_1t+\cdots+a_nt^n= Q(t)\in K[t]$, then for $f\in \mathscr{L}(V;V)$ we define 
$$Q(f):=a_0id_V+a_1f+a_2f^{\circ 2}+\cdots+a_nf^{\circ n}$$
\begin{itemize}
    \item[Remark]
From now on we write
$$f^{\circ k}=f^k$$
\end{itemize}
these are operations in $\mathscr{L}(V;V),+,\circ$

we say that $Q$ annihilates $f$ if $Q(f)=0$
\section{Prop}
\label{Prop.48.17}
Let $f\in \mathscr{L}(V;V)$. There exists a polynomial $Q\in K[t]\setminus\{0\}$ that annihilates $f$ (i.e. $Q(f)=0$)
\subsection*{Remark}The proof of this proposition also gives the degree of a polynomial that annihilates $(\leq n^2)$
\section{Def}
Let $m(t)\in K[t]\setminus\{0\}$ be a monic polynomial of minimal degree that annihilates $f\in \mathscr{L}(V;V)$. Then $m(t)$ is called minimal polynomial of $f$
And by prop above (\ref{Prop.48.17}), $m(t)$ exists.
\section{Prop}
If $m(t)$ is minimal polynomial of $f$, then $m(t)$ is unique.
\section{Prop}
Let $Q\in K[t]\setminus\{0\}$ be a polynomial that annihilates $f$. Then $m_f\mid Q$ 
\section{Theorem: Cayley-Hamilton Theorem}
The characteristic polynomial $P_f$ annihilates $f$
\section{Theorem}Let $f\in\mathscr{L}(V;V)$ when $V$ is a vector space of dim $n$, over an algebraically closed field.\\Then
\begin{itemize}
    \item [(1)]$f$ can be represented by a Jordan matrix
    \item [(2)]This above matrix is unique up to permutation of the Jordan blocks
\end{itemize}
\section{Def}
Let $f\in\mathscr{L}(V;V)$ and let $\lambda\in K$. A vector $w\in V\setminus\{0\}$ is called a root vector of $f$ corresponding to $\lambda$, if there exists $\mathbcal{r}\in\mathbb{N}$ s.t. $$(f-\lambda id_V)^\mathbcal{r}(w)=0$$
\subsection*{Remark}
Eigenvector are root vectors (corresponding to their eigenvalues) take $\mathbcal{r}=1$

\subsection*{Remark}
Let $J_\mathbcal{r}(\lambda)$ be a Jordan block. Then any $\sigma\in V$ is a root vector of $f$ corresponding to $\lambda$. In fact:
$$(J_\mathbcal{r}(\lambda)-\lambda I_n)^m=0\quad\text{if }m\geq\mathbcal{r}$$
\section{Prop}
\label{Prop 48.27}
Let $K$ be an algebraically closed field. Let $\lambda_1,\cdots,\lambda_k$ be all of distinct eigenvalues of $f(k\geq1)$, then 
$$V=\bigoplus\limits_{i=1}^kV(\lambda_i)$$
\section{Def}
Let $f\in  \mathscr{L}(V;V)$. Then $f$ is said to be nilpotent if there exists $t\in \mathbb{N}$ that $f^t=0$
\section{Lemma}
Let $f$ be a nilpotent mapping, then $Ker (f)\neq\{0\}$
\subsection*{Proof}
Let $\mathbcal{r}$ be the minimal integer s.t. $f^\mathbcal{r}=0$ then
$$f^{\mathbcal{r}-1}(V)\subseteq Ker(f)$$
but $f^{\mathbcal{r}-1}(V)\neq\{0\}$ because of the minimality of $\mathbcal{r}$
\section{Theorem}
Let $f\in\mathscr{L}(V;V)$ be a nilpotent mapping, then there exists a Jordan basis for $f$ that gives a Jordan matrix made of blocks of the type $J_\mathbcal{r}(0)$
\section{Theorem}
Let $K$ be an algebraically closed field. Let $f\in \mathscr{L}(V)$. Then  $f$ admits a Jordan basis (namely there exists a basis s.t. $A_f$ is a Jordan matrix). 
\section{Def}
Let $\lambda$ be an eigenvalue of $f\in \mathscr{L}(V)$
$$E(\lambda):=\ker (f-\lambda Id)$$
This $E(\lambda)$ is called the eigenspace of $\lambda$
$$mult(\lambda)_{geo}=dim(E(\lambda))$$
is called the geometric multiplicity of $\lambda$

Moreover
$$mult(\lambda)_{alg}=\max\left\{k\in \mathbb{N}\left| (t-\lambda)^k\mid P_f(t)\right.\right\}$$
is called the algebraic multiplicity of $\lambda$
\section{Prop}
Let $K$ be algebraically closed. Then $\forall \lambda$ eigenvalues of $f$
$$mult(\lambda)_{geo}\leq mult(\lambda)_{alg}$$
\section{Corollary}
Let $K$ be an algebraically closed field. Let $f\in \mathscr{L}(V)$. $f$ is diagonalizable iff $$\forall\lambda_i\quad mult(\lambda_i)_{geo}=mult(\lambda_i)_{alg}$$
\section{Def}Two matrices $G,G'\in M_{n\times n}(K)$ are said conjugate if $\exists A\in \mathcal{Q}_{n\times n}(K)$ s.t. $G=G'^T$
\section{Def}

Let $p\in \mathbb{R}^n$ be a fixed point
$$\mathbb{R}^n_p:=\{p\}\times\mathbb{R}^n$$
$(p,a)\in \mathbb{R}^n_p,a\in \mathbb{R}^n$
$$\begin{aligned}
    &(p,a)+(p,b)=(p,a+b)\\ &\alpha(p,a)=(p,\alpha a)\  \alpha\in \mathbb{R}
\end{aligned}$$

With these operation $\mathbb{R}^n_p$ is a vector space, which is called the tangent space of $\mathbb{R}^n$ at $p$.

The dual space is $$(\mathbb{R}^n_p)^\vee=\{p\}\times(\mathbb{R}_p^n)^\vee$$
A basis of $\mathbb{R}_p^n$ is denoted by $$(e_1\mid_p,\cdots,e_n\mid_p)$$

$\bigsqcup\limits_p \mathbb{R}^n_p$ is called the tangent bundle of $\mathbb{R}^n$
\subsection{Notation}
$$a\mid_p:=(p,a)$$
\section{Def}

Let $p\in \mathbb{R}^n$ be a fixed point
$$\mathbb{R}^n_p:=\{p\}\times\mathbb{R}^n$$
$(p,a)\in \mathbb{R}^n_p,a\in \mathbb{R}^n$
$$\begin{aligned}
    &(p,a)+(p,b)=(p,a+b)\\ &\alpha(p,a)=(p,\alpha a)\  \alpha\in \mathbb{R}
\end{aligned}$$

With these operation $\mathbb{R}^n_p$ is a vector space, which is called the tangent space of $\mathbb{R}^n$ at $p$.

The dual space is $$(\mathbb{R}^n_p)^\vee=\{p\}\times(\mathbb{R}_p^n)^\vee$$
A basis of $\mathbb{R}_p^n$ is denoted by $$(e_1\mid_p,\cdots,e_n\mid_p)$$

$\bigsqcup\limits_p \mathbb{R}^n_p$ is called the tangent bundle of $\mathbb{R}^n$

We have a projection mapping:
$$ 
\begin{aligned}
    &\bigsqcup \limits_{p}\mathbb{R}^n_p&\stackrel{\pi}{\twoheadrightarrow}&\mathbb{R}^n_p\\ &(p,a)&\mapsto &p
\end{aligned}$$
and 
$$
\begin{aligned}
    \mathbb{R}^n\times\mathbb{R}^n &\cong \bigsqcup \limits_{p}\mathbb{R}^n_p\\ (p,a) &\reflectbox{\ensuremath{\mapsto}} (p,a)
\end{aligned}
$$

Take $\{e_1\mid_p,\cdots,e_n\mid_p\}$ as a basis of $\mathbb{R}^n_p$. The dual basis is denoted by $$\{dx_1\mid_p,\cdots,dx_n\mid_p\}=\{(e_1\mid_p)^\vee,\cdots,(e_n\mid_p)^\vee\}\in (\mathbb{R}_p^n)^\vee$$
$$\begin{aligned}
    dx_i\mid_p: &\mathbb{R}^n_p &\rightarrow &\mathbb{R}\\
    &v=(\sum\alpha_ie_i\mid_p)&\mapsto &\alpha_i
\end{aligned}$$
$$\frac{\partial x_i}{\partial x_j}=dx_i\mid_p(e_j\mid_p)=\left\{\begin{aligned}
    &1&\text{ if }i=j\\
    &0&\text{ if }i\neq j
\end{aligned}\right.$$
Recalled the wedge algebra:
$$\bigwedge(\mathbb{R}^n_p)^\vee:=T(\mathbb{R}^n_p)^\vee/I=\bigoplus\limits_{k\in \mathbb{N}}\bigwedge\limits^k(\mathbb{R}_p^n)^\vee$$
Consider $$\bigwedge\limits^k(\mathbb{R}_p^n)^\vee$$
what's a basis of this vector space?
$$\left\{dx_1\mid_p\wedge\cdots\wedge dx_k\mid_p\textbf{\big|}1\leq i_1<\cdots< i_k\leq n\right\}$$
and $$\dim(\bigwedge\limits^k(\mathbb{R}_p^n)^\vee)={n\choose k}$$
Proved.
\section{Do Carmo Differential forms}
\section{Def}
An exterior $k$-form in $\mathbb{R}^n$ is a mapping:
$$\begin{aligned}
    \omega: &\mathbb{R}^n &\rightarrow &\bigsqcup\limits_{p}\bigwedge\limits^k(\mathbb{R}_p^n)^\vee\\
    &p&\mapsto&\omega(p)
\end{aligned}$$
that's a section of the projection $\pi$
$$(\pi\circ \omega=id_\mathbb{R})=(\omega(p)\in \bigwedge\limits^k(\mathbb{R}^n_p)^\vee)$$
$$\omega(p)=\sum\limits_{1\leq i_1<\cdots<i_k\leq n}a_{i_1,\cdots,i_k}(p)dx_{i_1}\mid_p\wedge\cdots\wedge dx_{i_k}\mid_p\in \bigwedge\limits^k(\mathbb{R}_p^n)^\vee$$
Note that
$$\begin{aligned}
    &\bigsqcup\limits_{p}\bigwedge\limits^k(\mathbb{R}_p^n)^\vee&\stackrel{\pi}{\twoheadrightarrow}&\mathbb{R}^n\\
    &f\mid_p&\mapsto&p 
\end{aligned}$$
$$\omega\leftrightarrow\{a_{i_1},\cdots,a_{i_k}\}$$
if all $a_{i_j}$ are of class $C^m(\mathbb{R})$ the $\omega$ is called a $C^m$-differential $k$-form. If $m=+\infty$ $omega$ is called a smooth $k$-form.
\section{Notation}
$$\omega=\sum\limits_{I}a_Idx_I$$
where $I=(i_1,\cdots,i_k)$
\section{Notation}
When $k=0$ a $0$-form of class $C^m$-differential 0-form is $f\in C^m(\mathbb{R}^n)$$$C^m(\mathbb{R}^n)=\{f:\mathbb{R}^n\rightarrow \mathbb{R}\text{ of class }C^m\}$$
\section{Notation}
$$\Omega^k_{(m)}(\mathbb{R}^n):=\{\text{set of }C^m\text{-diff }k\text{-forms}\}$$
$$\Omega^0_{(m)}(\mathbb{R}^n)=C^m(\mathbb{R}^n)$$
$m$ could be omitted if no confusion.
\section{Def}
Now we have $$\Omega(\mathbb{R}^n)=\bigoplus\limits_{k\in \mathbb{N}}\Omega^k(\mathbb{R}^n)$$ a $\mathbb{R}$-algebra with the $\wedge$-product

And it's also a $\Omega^0(\mathbb{R}^n)$ module and $\Omega^0(\mathbb{R}^n)$-algebra

\section{Def: Pullback of forms}
Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ be a mapping of $C^\mathbcal{r}$, then it induces a mapping 
$$\begin{aligned}
    f^*:\Omega^k_{(\mathbcal{r})}(\mathbb{R}^m)&\rightarrow\Omega^k_{(\mathbcal{r})}(\mathbb{R}^n)\\\omega&\mapsto f^*\omega
\end{aligned}$$
and $$f^*(\omega)(p)(v_1,\cdots,v_k)=\omega(f(p))(df\mid_p(v_1),\cdots,df\mid_p(v_k))$$
recalling$$df\mid_p:\mathbb{R}^n_p\rightarrow\mathbb{R}^m_{f(p)}\ \Rightarrow\ df\mid_p(v_i)\in \mathbb{R}^n_{f(p)}$$
\section{Remark}
$f\in \Omega^0(\mathbb{R}^n),\omega\in \Omega^k(\mathbb{R}^n)$$$f\wedge\omega=f\omega$$
\section{Prop}
Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a differentiable mapping. Then
\begin{itemize}
    \item [(1)]for any two forms in $\mathbb{R}^m$$$f^*(\omega\wedge\eta)=(f^*\omega)\wedge(f^*(\eta))$$
    \item [(2)]for $g:\mathbb{R}^p\rightarrow\mathbb{R}^n$ differentiable$$(f\circ g)^*\omega=g^*(f^*\omega)$$
\end{itemize}
\section{Def: Path integral}
\label{Def 52.2}
Let $\gamma$ and $\omega$ be as above.
$$\int_\gamma\omega:=\sum\limits_i\int_{t_k}^{t_{k+1}}\gamma_j^*\omega$$
this is the integral of $\omega$ along the parametric curve $\gamma$ with
$$\gamma=t\mapsto(x_1(t),\cdots,x_n(t))$$
where $x_i(t)=\frac{\text{d}x_i}{\text{d}t}$
\section{Def($\sigma$-finite)}
Let $(X,\Sigma_X,\mu)$ be a measure space. WE say that it's $\sigma$-finite if there exists a sequence $\{E_n\}_{n\in \mathbb{N}}$ of measurable sets. (namely $E_n\in \Sigma_X$) such that $$X=\bigcup\limits_{n\in \mathbb{N}}E_n\text{ and }\mu(E_n)<+\infty, \forall n\in \mathbb{N}$$
\section{Notation}
Take sets $A\subseteq X\times Y$ For $x\in X$, we define
$$A_x:=\{u\in Y\mid(x,y)\in A\}$$
called a \textbf{vertical section} of $A$ or $x-$fiber of $A$

For $y\in Y$ we define
$$A_y:=\{x\in X\mid(x,y)\in A\}$$
called a \textbf{horizontal section} of $A$, or $y$-fiber of $A$
\section{Def}
Let $X$ be a set. then $\mathscr{D}\subseteq\wp(X)$ is a \textbf{Dynkin system} if 
\begin{itemize}
    \item $X\in \mathscr{D}$ and $\varnothing\in \mathscr{D}$
    \item $\forall D\in \mathscr{D}\quad X\setminus D\in \mathscr{D}$
    \item If $\{D_n\}_{n\in \mathbb{N}}$ is a sequence in $\mathscr{D}$ of pairwise disjoint sets, then $$\bigsqcup\limits_{n\in \mathbb{N}}D_n\in \mathscr{D}$$
\end{itemize}
\section*{Remark}
A $\sigma$-algebra is a Dynkin system
\section{Def}
Let $(\mathcal{G}\subseteq\wp(X))$ then $\delta(\mathcal{G})\subseteq\wp(X)$ is called the Dynkin system generated by $\mathcal{G}$ if \begin{itemize}
    \item $\mathcal{G}\subseteq\delta(\mathcal{G})$
    \item If $\mathscr{D}$ is a Dynkin system containing $\mathcal{G}$, then $\delta(\mathcal{G})\subseteq\mathscr{D}$
\end{itemize}
\section{Prop}
If $\mathscr{D}$ is a Dynkin system closed under the intersection, then it's a $\sigma$-algebra, namely
$$\forall(D,E)\in \mathscr{D}^2, D\cap E\in \mathscr{D}\ \Rightarrow\ \forall\{D_n\}_{n\in \mathbb{N}}\in \mathscr{D}^\mathbb{N}\quad \bigcup\limits_{n\in \mathbb{N}}D_n\in \mathscr{D}$$
\section{Prop}
\label{Prop 53.7}
Let $X$ be a set and let $\mathcal{G}\subseteq\wp(X)$. Assume that $\mathcal{G}$ is closed under the finite intersection. Then $$\delta(\mathcal{G})\subseteq\sigma(\mathcal{G})$$
\section{Theorem}
Let $(X,\Sigma_X,\mu)$ and $(Y,\Sigma_Y,\nu)$ be $\sigma$-finite measure spaces. Then $\forall E\in \Sigma_X\otimes\Sigma_Y$, the functions
$$\begin{aligned}
    f_E: &X &\rightarrow &\mathbb{R}\cup\{+\infty\}\\ &x &\mapsto &\nu(E_x)\\
    g_E: &Y &\rightarrow &\mathbb{R}\cup\{+\infty\}\\ &y &\mapsto &\mu(E_y)
\end{aligned}$$
are respectively $\Sigma_X$-measurable and $\Sigma_Y$-measurable
\section{Theorem}
Let $(X,\Sigma_X,\mu)$ and $(Y,\Sigma_Y,\nu)$ be $\sigma$-finite measure spaces. There exists a unique $\sigma$-finite measure $\mu\times\nu$ on $(X\times Y,\Sigma_X\otimes\Sigma_Y)$ such that
$$\mu\times\nu(S_1\times S_2)=\mu(S_1)\nu(S_2)\quad \forall(S_1,S_2)\in\Sigma_X\times \Sigma_Y$$
and moreover, we have 
$$(\mu\times\nu)(E)=\int_Xf_E\text{d}\mu=\int_Yg_E\text{d}\nu$$
\section{Def: Push-forward measure}
Let $(X,\Sigma_X,\mu)$ be a measure space, and let $(Y,\Sigma_Y)$ be a measurable space. If $f:X\rightarrow Y$ is a measurable function, then define:
$$f_{*\mu}(E)=\mu(f^{-1}(E))\quad \forall E\in \Sigma^Y$$
This is a measure on $Y$, called the push forward of $\mu$ through $f$
\section{Fubini-Tobelli Theorem}
Let $(X,\Sigma_X,\mu)$ and $(Y,\Sigma_Y,\nu)$ be two $\sigma$-finite measure spaces. Let $(X\times Y,\Sigma_X\otimes\Sigma_Y,\mu\times\nu)$ be the product space. Let $f:X\times Y\rightarrow \mathbb{R}\cup\{-\infty,+\infty\}$ be a measurable function. Then 
$$\begin{aligned}
    \int_{X\times Y}\abs{f}\text{d}(\mu\times\nu) &=\int_X(\int_Y\abs{f(x,y)}\text{d}\nu(y))\text{d}\mu(x)\\
    &=\int_X(\int_Y\abs{f(x,y)}\text{d}\nu(y))\text{d}\mu(x)
\end{aligned}$$
\section{Notation}
For any mapping $\gamma:[a,b]\rightarrow U$
\begin{itemize}
    \item $\gamma$ is called a closed curve if $\gamma(a)=\gamma(b)$ and $\gamma$ is a curve
    \item $\gamma$ is called a path if $\gamma$ is of class $C^0$
    \item $\gamma$ is called a loop if $\gamma$ is a closed path
\end{itemize}
\section{Def: Lebesgue number}
Let $(X,\rho)$ be a metric space and $\mathcal{U}=\{U_i\}$ be an open covering $X$

A \textbf{Lebesgue number} $\delta=\delta_{\mathcal{U}}$ (of the open covering $\mathcal{U}$) is a non-negative number that:

If $Z\subseteq X$ is a subset with $diam(Z)<\delta$, then $Z\subseteq U_j$ for some $U_j\in \mathcal{U}$
\subsection*{Remark}\begin{itemize}
    \item $\delta'<\delta$ is also a Lebesgue number
    \item In principle, a Lebesgue number $\delta$ can be 0
\end{itemize}
\section{Lemma} If $X$ is compact, then for any open covering there exists a positive Lebesgue number.
\section{Theorem(homotopy invariance of the integrals)}
Ler $\omega$ be a closed form on an open set $U$. Let $\gamma_0,\gamma_1$ be homotopy paths in $U$, then $$\int_{\gamma_0}\omega=\int_{\gamma_1}\omega$$
\section{Def: Free Homotopy}
Let $\gamma_0,\gamma_1:[a,b]\rightarrow U$ be two loops (namely $\gamma(a)=\gamma(b)$)

A \textbf{free homotopy} between $\gamma_0$ and $\gamma_1$ is a continuous mapping:
$$\begin{aligned}
    H:&[a,b]\times[0,1]&\rightarrow&U\\
    &(s,t)&\mapsto&H(s,t)
\end{aligned}$$
such that\begin{itemize}
    \item $$H(\cdot,0)=\gamma_0\quad H(\cdot,1)=\gamma_1$$
    \item For any fixed $t_0$$$H(\cdot,t_0)$$ is a loop
\end{itemize}
\section{Notation}
A path $\gamma:[a,b]\rightarrow I$ is said simple if $\gamma\mid_{\rightbracket a,b\leftbracket}$ is injective (No self-cross this is)
\section{Jordan Theorem}
Let $\gamma$ be a simple loop $\gamma:[a,b]\rightarrow U$ , then $\mathbb{R}^2\setminus\gamma([a,b])$ consists exactly of two connected components. One of this is bounded (interior), the other one unbounded (exterior). Moreover $\gamma([a,b])$ is the boundary of two components.
\section{Def}
Let $c:[a,b]\rightarrow S^1$ be a closed curve. Let $\varphi$ be the angular function of $c$. We define the winding number of $c$ as:
$$n(c)=\frac{1}{2\pi}(\varphi(b)-\varphi(a))$$
Since $c$ us a closed curve, $n(c)\in \mathbb{Z}$
\section{Def}
Let $\gamma:[a,b]\rightarrow\mathbb{R}^2\setminus\{p\}$ be a closed curve. ($\gamma_p+\rho(t)c(t)$), when $c(t)\in S^1$$$\gamma(t)=p+\rho(t)(\cos(\theta(t))+\sin(\theta(t)))$$
Then we define the winding number of $\gamma$ at $p$
$$n_p(\gamma):=n(c)$$
\section{Prop}
Let $\gamma=p+\rho(t)c(t)$ be a closed curve $\gamma:[a,b]\rightarrow \mathbb{R}^2\setminus\{p\}$ then
$$n_p(\gamma)=\frac{1}{2\pi i}\int_C\omega_0$$
where $$\omega_0=-\frac{y}{x^2+y^2}\text{d} x+\frac{x}{x^2+y^2}\text{d}y$$
\section{Prop}
Let $\gamma_0,\gamma_1:[0,b]\rightarrow\mathbb{R}^2\setminus\{p\}$ be two closed curves. Then they're freely homotopic iff$$n_p(\gamma_0)=n_p(\gamma_1)$$
\section{Def}
Let $F:U\subseteq \mathbb{R}^2\rightarrow\mathbb{R}^2$ be a differential mapping. We say that $p\in U$ is a zero of $F$ if $F(p)=0$. If then exists a neighborhood $V$ of $p$ such that $V$ contains no zero of $F$ other then $p$,  then $p$ is called isolated zero.

If $p$ is a zero of $F$ and $\text{d}F\mid_p$ is non singular at $p$, then we say that $p$ is a simple zero.
\section{Def} The index of $F$ in $D$, is defined as
$$n(F,D):=\frac{1}{2\pi}\int_C\theta$$
See that $\theta=F^*\omega_0$, $\omega_0=\frac{-y\text{d}x+x\text{d}y}{x^2+y^2}$
$$\begin{aligned}
    n(F,D)&=\frac{1}{2\pi}\int_C\theta\\
    &=\frac{1}{2\pi}\int_CF^*\omega_0\\
    &=\frac{1}{2\pi}\int_{F\circ}\omega_0\\
    &=(\text{winding number of }F\circ C\text{ at the center of }F D)
\end{aligned}$$ 
\section{Remark}
$$n(F,D)=\frac{1}{2\pi}\int_C\theta=\frac{1}{2\pi}\int_{F\circ C}\omega_0$$
\section{Prop}
If $n(F,D)\neq 0$ then $\exists q\in D$ s.t. $F(q)=0$
\section{Def}
A simple zero $p$ of $F$ is said \textbf{positive} if $\det(\text{d}_p F)>0$, otherwise is said \textbf{negative} ?(what's =0?)
\section{Kronecker Index Theorem}
Assume that $F;U\subseteq \mathbb{R}^2\rightarrow\mathbb{R}^2$ has only finite simple zeros in a disk $D\subseteq U$ and none of them in $\partial D$. Then
$$n(F,D)=P-N$$
where $P$ is the number of positive simple zeros and $N$ is the number of negative simple zeros.
\section{Def}
Let $\mathcal{P}=\{t=t_a,t_1,\cdots,t_n=b\}$, $p_i=\gamma(t_i)$
$$l_{\mathcal{P}}(\gamma)=\sum\limits_{i=0}^n\norm{p_{i+1}-p_i}$$
The length of $\gamma$ is 
$$l(\gamma):=\sup\limits_p\{l_p(\gamma)\}$$
If $l(\gamma)<+\infty$, then path $\gamma$ is said rectifiable.
\section{Prop}
Let $\gamma:[a,b]\rightarrow\mathbb{R}^n$ be of class $C^1$, then $\gamma$ is rectifiable and $$l(\gamma)=\int_a^b\norm{\gamma'(t)}\text{d}t$$
moreover $l(\gamma)$ doesn't depend on the parametrization of $\gamma$
\section{Corollary(exercise)}
If $\gamma$ is a curve (piecewise $C^1$), then $\gamma$ is rectifiable and the length is the sum of the length of it's $C^1$ pieces.
\section{Def}
A $C^1$-curve is \textbf{regular} if $\gamma'(t)\neq0$ for any $t\in [a,b]$ A piecewise $C^1$-path (curve) is regular if all its pieces are regular



\section{Def}
$$N:=\frac{T}{\norm{T}}$$ is \textbf{normal vector} of $T$
\section{Def}
Let $\gamma:[a,b]\rightarrow\mathbb{R}^n$ a $C^1$ curve; Let $l$ be the length of $\gamma$ (by theorem proved $l(\gamma)<+\infty$) Let's define the following function:
$$s(t):=\int_a^t\norm{\gamma'(t)}\text{d}u$$
$s(t)$ is the length of $\gamma\mid_{[a,t]}$
THe function $\norm{\gamma'(u)}$ iss continuous, hence
$$s'(t)=\norm{\gamma'(t)}$$
Now assume that $\gamma$ is $C^1$ and \textbf{regucar}($\gamma'(t)\neq0,\forall t\in [a,b]$), then $s'(t)>0$

So $s:[a,b]\rightarrow[0,l]$ is a $C^1$-differmorphism, the inverse is $$t:[0,l]\rightarrow[a,b]$$$$\frac{\text{d}t}{\text{d}s}=frac{1}{\norm{\gamma'(t)}}$$
We reparameterize $\gamma$ with $t$ and get
$$\tilde{\gamma}(s)=(\gamma\circ t)(s)$$
$\tilde{\gamma}:[0,l]\rightarrow\mathbb{R}^n$ we say that $\tilde{\gamma}$ is the reparameterization of $\gamma$ with respect to its \textbf{curvilinear coordinate} $s(t)$
\section{Def}
$f:U\subseteq\mathbb{R}^n\rightarrow\mathbb{R}^n$ $f$ is a $C^{(k)}$-differ if \begin{itemize}
    \item $f$ is of class $C^{(k)}$
    \item $f$ is bijection, and the inverse is $C^{(k)}$
\end{itemize}
\section{Def}
In general $$\gamma:[a,b]\rightarrow\mathbb{R}^n\leadsto \tilde{\gamma}:[0,l]\rightarrow\mathbb{R}^n$$
regular and $C^1$
$$\frac{\text{d}\tilde{\gamma}}{\text{d}s}=\frac{\text{d}\gamma}{\text{d}t}\frac{\text{d}t}{\text{d}s}=\frac{\gamma'(t)}{\norm{\gamma'(t)}}$$
$\norm{\frac{\text{d}\tilde{\gamma}}{\text{d}s}}=1$
$$T(t):=\frac{\text{d}\tilde{\gamma}}{\text{d}s}=\frac{\gamma'(t)}{\norm{\gamma'(t)}}$$
tangent: (vector) $\rightarrow$ vector of norm 1
$$0=\frac{\text{d}}{\text{d}t}\norm{T(t)}^2=\frac{\text{d}}{\text{d}t}\left<T(t),T(t)\right>=2\left<T(t),T'(t)\right>\Leftrightarrow T'(t)\perp T(t)$$
use the fact that in $\mathbb{R}^n$, $u,v:\mathbb{R}\rightarrow\mathbb{R}^n$ differentiable
$$\begin{aligned}
    \frac{\text{d}}{\text{d}t}\left<u(t),v(t)\right> &= \left<\frac{\text{d}u}{\text{d}t},v(t)\right>+\left<u(t),\frac{\text{d}v}{\text{d}t}\right>
\end{aligned}$$
then
$$
\begin{aligned}
    \frac{\text{d}^2\tilde{\gamma}}{\text{d}s^2}&=\frac{\text{d}}{\text{d}s}(\frac{\text{d}\tilde{\gamma}}{\text{d}s})\\
    &=\frac{\text{d}}{\text{d}s}(T(t))\\
    &=\frac{\text{d}T}{\text{d}t}\frac{\text{d}t}{\text{d}s}\\
    &=\frac{T'(t)}{\norm{\gamma'(t)}}    
\end{aligned}$$
$N(t)=\frac{\text{d}^2\tilde{\gamma}}{\text{d}s^2}/\norm{\frac{\text{d}^2\tilde{\gamma}}{\text{d}s^2}}$. If $n=2$
Along the curve we have a 'moving' canonical basis of 
$$\begin{aligned}
    \mathbb{R}^2_{\gamma(t)}&=\span\{T(t),N(t)\}\\
    &=\{\alpha T(t)+\beta N(t)\mid\alpha,\beta\in \mathbb{R}\}
\end{aligned}$$
$\{T(t),N(t)\}$ is a orthonormal basis of $\mathbb{R}^2_{\gamma(t)}$
\section{Def:isometry}
$$(V,g)\stackrel{f}\rightarrow(W,g')$$
a morphism $f$ of vector space with inner product is \textbf{isometry} if $$g(x,y)=g'(f(x),f(y))$$
\section{Def:isometric}
$V\stackrel{\cong}\rightarrow W$ up to isomorphism.

Then $(V,g)$ and $(W,g')$ are \textbf{isometric} if there are two isometry
$$\begin{aligned}
    f:(V,g)&\rightarrow&(W,g')\\
    f':(W,g')&\rightarrow&(V,g)
\end{aligned}$$
such that
$$f\circ f'=f'\circ f=Id$$
\section{Def: Semilinear}
If $V$ and $W$ are two complex vector sapce, then a \textbf{semilinear mapping} is a mapping $f:V\rightarrow W$ such that
\begin{itemize}
    \item $f(v_1+v_2)=f(v_1)+f(v_2)$
    \item $f(\alpha v)=\alpha*f(v)=\overline{\alpha}f(v)$
\end{itemize}
So a semilinear mapping is a linear mapping: $f:V\rightarrow W$

For sesquilinear forms, the theory is similar to the theory of bilinear forms.
$$g\leadsto G(\text{fix a basis})\quad g(x,y)=xG\overline{y}$$
If you change basis, then the Gram matrix changes in the following way:
$$G\leadsto A^TG\overline{A}$$
If $g$ is bilinear
$$g\leadsto \tilde{g}:V\rightarrow V^\vee$$
and $$g\leadsto\tilde{g}:V\rightarrow\overline{V^\vee}$$
linear if $g$ is sesquilinear ($\tilde{g}:V\rightarrow V^\vee$ is semilinear)
\section{Def}
A sesquilinear form $g:V\times \overline{V}\rightarrow K$ is \textbf{hermitian} if
$$g(x,y)=\overline{g(y,x)}$$
And note that inner product is any of symmetric symplectic or hermitian.

\chapter{Classification (up to isometry) of vector spaces of small dim}
Let $(V,g)$ be vector space over $K(=\mathbb{R},\mathbb{C})$ with inner product.
\section{$\dim V=1$ and $g$ is symmetric}
choose $v\in V\setminus\{0\}$ if $g(v,v)=0$, then $g$ is degenerated $\Rightarrow\ g=0$

If $g$ is non-deg (non-degenerate) $\exists v$ s.t. $g(v,v)=a\neq0$$$\forall x\in K\quad g(xv,xv)=ax^2$$

Any $v$ s.t. $g(v,v)=a\neq0$ induce a set
$$\mathcal{C}(v):=\{ax^2:x\in K^*\}$$ this is an element in $K^*/\{x^2\mid x\in K^*\}$
\subsection{Prop}Let $(V_1,g_1),(V_2,g_2)$ be two vector spaces of $\dim\ 1$ s.t. $g_1$ and $g_2$ are symmetric. Then $(V_1,g_1)$ and $(V_2,g_2)$ are isometric iff
$$\exists v_1\in V_1,v_2\in V_2\text{ s.t. }\mathcal{C}_{g_1}(v_1)=\mathcal{C}_{g_2}(v_2)$$
\subsection{Theorem}
$(V,g)$ has dim 1, $g$ symmetric. Then $(V,g)$ is isometric to one of the following
\begin{itemize}
    \item $K=\mathbb{R}$$$(\mathbb{R},g(x,y)=xy)\quad(\mathbb{R},g(x,y)=-xy)\quad (\mathbb{R},g(x,y)=0)$$
    \item $K=\mathbb{C}$$$(\mathbb{C},g(x,y)=xy)\quad (\mathbb{C},g(x,y)=0)$$ 
\end{itemize}
\section{$\dim V=1$ $g$ is hermitian}
Again $g$ degenerate $\Rightarrow g=0$We use that same reason as above. $v\in V:g(v,v)=a\neq 0$, $\forall a\in \mathbb{C}^*$$$g(av,av)=\norm{a}^2g(v,v)$$
So any element $v\in V\setminus\{0\}$ s.t. $g(v,v)=a$ induces a coset in $\mathbb{C}^*/\mathbb{R}_{> 0}$

Inside $\mathbb{C}^*$, $\mathbb{R}_{>0}$ is a (mult) subgroup

For any $z\in \mathbb{C}^*$ can be written uniquely as $z=re^{i\theta}$, hence
$$\begin{aligned}
    \mathcal{C}^* &\cong&\mathbb{R}_{>0}\times S^1&\rightarrow& S^1\\
    z& \mapsto&(r,e^{i\theta}) &\mapsto& e^{i\theta} 
\end{aligned}$$
The kernel is $\mathbb{R}_{>0}$ and $S^1\cong\mathbb{C}^*/\mathbb{R}_{>0}$.

But $g$ is hermitian, so$$g(v,v)\in \mathbb{R}$$
If follows that the coset $$\{\norm{a}^2g(v,v)\mid a\in  \mathbb{C}^*\}\in (\mathbb{C}^*/\mathbb{R}_{>0})\cap(\mathbb{R}/\mathbb{R}_{>0})\cong\{\pm1\}$$
We repeat the proposition before with $g$ hermitian and the following theorem
\subsection{Theorem} $(V,g)$ if dim 1, with $g$ hermitian. Then $(V,g)$ is isometry to one of the following
$$(\mathbb{C},g(x,y)=x\overline{y})\quad(\mathbb{C},g(x,y)=-x\overline{y})\quad(\mathbb{C},g(x,y)=0)$$
\section{$\dim V=1$ $g$ symplectic}
With dim 1 $\forall v_1,v_2\in V$ can be write as $v_1=ae,v_2=be$ with $e\in K^*$$$g(v_1,v_2)=ab\cdot g(v,v)=0$$
\subsection{Theorem}$(V,g)$ of dim 1, $g$ symplectic, then $$(V,g)\cong(K,g=0)$$
\section{$\dim V=2$ $g$ symplectic}
Assume that $g$ is degenerated, then $\exists x\in V$ s.t. $g(x,y)=0,\forall y\in V$

Extend $x$ to a basis $\{x,x'\}$ of $V$
$$g(ax+a'x',bx+b'x')=ab\cdot g(x,x)+ab'\cdot g(x,x')-a'b\cdot g(x,x')+a'b'\cdot g(x',x')=0$$

So when $g$ degenerated $g=0$


Take $g$ non-degenerated $\exists v_1,v_2\in V$ s.t. $g(v_1,v_2)=a\neq0$. 

For $g(a^{-1}v_1,v_2)=a^{-1}a=1$, we may assume that $a=1$

Let's show that $v_1,v_2$ are linearly independent. Assume by contraction: $v_1=\lambda v_2$
$$1=g(v_1,v_2)=g(\lambda v_2,v_2)=\lambda\cdot g(v_2,v_2)=0$$
$\Rightarrow$ $\{v_1,v_2\}$ is a basis of $V$. Then
$$\alpha_1\beta2-\alpha_2\beta_1=g(\alpha_1v_1+\alpha_2v_2,\beta_1v_1+\beta_2v_2)=(\alpha_1,\alpha_2)\begin{pmatrix}
    0 &1\\-1&0
\end{pmatrix}\begin{pmatrix}
    \beta_1\\\beta_2
\end{pmatrix}\stackrel{(?)}{=}\begin{pmatrix}
    \ \overline{\beta_1}\ \\\overline{\beta_2}
\end{pmatrix}$$
\subsection{Theorem}
$(V,g)$ is dim 2, $g$ symplectic. Then $(V,g)$ is isometric to one of the following
$$(K^2,g(x,y)=0)\quad(K^2,g(x,y)=x_1y_2-x_2y_1)$$
\chapter{Compliment}
\section{Def: non-degenerate}
Let $(V,g)$ be a inner product space. Let $V_0\subseteq V$ be a subspace. We say that $V_0$ is non-degenerate if $g\mid_{V_0}$ is non-degenerate.

Moreover, $V_0$ is isotropic if $g\mid_{V_0}=0$
\subsection*{Remark}Isotropic means degenerate
\section{Def}
Let $(V,g)$ be a inner product space. Let $V_0\subseteq V$ be a subspace. The orthogonal complement of $V_0$ is define as
$$V_0^\bot:=\{v\in V\mid g(v,v_0)=0,\forall v_0\in V_0\}$$
\section{Prop}
Let $(V,g)$ be a inner product space. Let $V_0\subseteq V$ be a non-degenerate subspace. Then$$V=V_0\oplus V_0^\bot$$
\section{Theorem}
Let $(V,g)$ be an finite dimensional inner product space. If both $V_0$ and $V_0^\bot$ are non-degenerate, then $(V^\bot)^\bot=V_0$
\section{Theorem}
\label{theorem 58.5}
Let $(V,g)$ be an finite dimensional inner product space. Then There exists a decomposition$$V=V_1\oplus\cdots\oplus V_n$$
such that $\{V_i\}_{i=1}^n$ are pairwisely orthogonal and \begin{itemize}
    \item [1] They are 1-dim if $g$ is symmetric or hermitian
    \item [2]They are 1-dim but degenerated or 2-dim non-degenerate if $g$ is symplectic.
\end{itemize}
\chapter{Signature}
Now we discuss the uniqueness of such decomposition
\section{Def}
Let $(V,g)$ be an inner product space with dim 1. Moreover, assume that $g$ is symmetric or hermitian. We say that $(V,g)$ is \textbf{positive} if $(V,g)$ is isometry to either $(\mathbb{R},g(x,y)=xy)$ or $(\mathbb{C},g(x,y)=x\overline{y})$

We say that $(V,g)$ is \textbf{negative} if $(V,g)$ is isometry to either $(\mathbb{R},g(x,y)=-xy)$ or $(\mathbb{C},g(x,y)=-x\overline{y})$
\section{Notation}
By theorem \ref{theorem 58.5}, we can count the number of positive subspace of any inner product space.
\begin{itemize}
    \item $r_0:=\dim\ker g$
    \item $r_+:=$ the number of positive subspaces
    \item $r_-:=$ the number of negative subspaces
\end{itemize}
\section{Def}
Let $(V,g)$ be an inner product space.
\begin{itemize}
    \item [1]If $g$ is real symmetric or, hermitian, then $(r_0,r_+,r_-)$ is signature of $V$
    \item [2]If $g$ is symplectic or complex symmetric, then $(\dim V,r_0)$ is the signature of $V$
\end{itemize}
\section{Theorem}
Let $(V,g)$ and $(V',g')$ be two inner product spaces, with $g,g'$ that are either (both) symplectic or complex symmetric.

Then $(V,g)$ and $(V',g')$ are isometric iff$$(n,r_0)=(n',r_0')$$

\section{Theorem}
Let $(V,g)$ and $(V',g')$ be two inner product spaces, with $g,g'$ that are either (both) hermitian or real symmetric.

Then $(V,g)$ and $(V',g')$ are isometric iff$$(r_0,r_+,r_-)=(r_0',r_+',r_-')$$
\chapter{Orthonormal}
\section{Def}
Let $(V,g)$ be an inner product space. The basis $\{v_1,\cdots, v_n\}$ is said \textbf{orthogonal} if $g(v_i,v_j)=0,\forall i\neq j$

Moreover, $g$ is said \textbf{orthonormal} if $g(v_i,v_i)\in \{0,-1,1\},\forall i$
\subsection*{Remark}
If $g$ is hermitian or symmetric. We can always find an orthonormal basis from an orthogonal basis.
\section{Def}
Let $V$ be a vector space over $K$($char K\neq 2$). A \textbf{quadratic form} on $V$ is a mapping $a:V\rightarrow K$ such that
\begin{itemize}
    \item $q(\alpha v)=\alpha^2q(v)\forall \alpha\in K,v\in V$
    \item $f=(u,v)\mapsto\ q(u+v)-q(u)-q(v)$ is bilinear
\end{itemize}
\subsection*{Remark}
Any symmetric bilinear form $h:V^2\rightarrow K$ is a quadratic form. Given a quadratic form $q:V\rightarrow K$, we can define a symmetric bilinear form$$h_p(u,v)=\frac{1}{2}\left(q(u+v)-q(u)-q(v)\right)$$
\section{Gram-Schmidt algorithm}
Let $(V,g)$ be an inner product space with $g$ symmetric or hermitian. Let $\{v'_1,\cdots,v'_n\}$ be a basis of $V$ such that $V_i=\left<v'_1,\cdots,v'_i\right>\ \forall i\in \{1,\cdots,n\}$ is non-degenerate.

Then there exists an orthogonal basis $\{v_1,\cdots,v_n\}$ such that $V_i=\left<v_1,\cdots,v_i\right>$ $\forall i\in \{1,\cdots,n\}$ is non-degenerate.
\chapter{Euclidean and Unitary Spaces}
\newcommand{\inprod}[2]{\left<#1,#2\right>}
\section{Def:Euclidean vector space}
A euclidean vector space is a finite dimensional inner product space over $\mathbb{R}$ $(E,g)$ with $g$ symmetric and positive definite ($g(x,x)>0,\forall x\neq 0$)

We denote$$\left<x,y\right>:=g(x,y)$$\
\subsection*{Remark}
Any non-zero subspace of $(E,\left<\cdot,\cdot\right>)$ is non-degenerated:

The signature of $E$ is of the type $(0,\mathbcal{r}_+,\mathbcal{r}_-)$ denoted by $(P,Q)(P=\mathbcal{r}_+,Q=\mathbcal{r}_-)$

An euclidean space is a normed vector space $$\norm{x}=\sqrt{\inprod{x}{x}}$$
($\inprod{}{}$ positive defined required)

Any euclidean vector admits an orthonormal basis $\{v_1,\cdots,v_n\},\inprod{v_i}{v_i}=1$ So orthonormal means $$\norm{v_1}=1$$
In a euclidean space we have a distance $$d(x,y)=\norm{x-y}$$
\section{Prop}
A euclidean space $(E,\inprod{}{})$ of dim $n$ is isometric to $(\mathbb{R}^n,\underbrace{\inprod{}{}}\limits_{\text{usual scalar product}})$
\section{Remark}
\begin{itemize}
    \item [Cauchy-Schwartz inequality]$$\inprod{x}{y}\leq\norm{x}\norm{y}$$
    \item [Triangle inequality]$$\norm{x+y}\leq\norm{x}+\norm{y}$$
\end{itemize}
\section{Pythagoras's Theorem}
\label{Pythagoras}
If $x_1,\cdots,x_k$ are pairwise orthogonal, then
$$\norm{\sum\limits_{i=1}^kx_i}^2=\sum\limits_{i=1}^k\norm{x_i}^2$$
\section{Def:Angles}
By Cauchy-Schwartz inequality:
$$-1\leq\frac{\inprod{x}{y}}{\norm{x}\norm{y}}\leq1$$
Then there exists a element $\phi\in [0,\pi]$ such that
$$\cos\phi=\frac{\inprod{x}{y}}{\norm{x}\norm{y}}$$
$\phi$ is defined as the angle between $x$ and $y$

Notice that $\phi$ is not an oriented angle.
\section{Notation}
Let $U,V\subseteq E$ be two subspace, then $$d(U,V):=\inf\{\norm{u-v}\mid u\in U,v\in V\}$$
\section{Def}
$V\subsetneq E$ a vector subspace, $x\in E\setminus\{0\}$. Then $E=V\oplus V^\perp$ (proved) Then we write (uniquely)
$$x=x_0,+x'_0$$ where $x_0\in V, x'_0\in V^\perp$.

Then $x_0$ is called the orthogonal projection of $x$ on $V$, $x'_0$ is called the orthogonal projection of $x$ on $V^\perp$
\section{Prop}
Use the above notation:
$$d(x,V)=\norm{x'_0}$$
\section{Prop}
Use the previous notations. Assume that $m=\dim V,\ V\subseteq E$, $\{v_1,\cdots,v_m\}$($m\leq n=\dim E$) is an orthonormal basis of $V$. Then
$$x_0=\sum\limits_{i=1}^m\inprod{x}{v_i}v_i$$
\section{Relationship with calculus}
$(E,\inprod{}{})=(\mathbb{R}^n,\inprod{}{})$ on $\mathbb{R}^n$ we have the notion of volumes$$vol(B):=\lambda^n(B)$$
where $B$ is a Borel set.

A $n$-dimensional parallelepiped is :
$$P_n=\left\{t_1v_1+\cdots+t_nv_n\mid t_i\in[0,1]\forall i\right\}$$
Consider a linear mapping
$$\begin{aligned}
    A_{P_n}=A:\mathbb{R}^n&\rightarrow&\mathbb{R}^n\\x&\mapsto&Ax
\end{aligned}$$
where $A\in\mathcal{M}_{n\times n}(\mathbb{R})$, $A=(v_1\mid\cdots\mid v_n)$

$A$ is invertible iff $\{v_1,\cdots,v_n\}$ is a basis. Let $\prod_n=[0,1]^n$ then$$A(\prod_n)=P_n$$
If $A$ invertible
$$\begin{aligned}
    vol(P_n)&=\lambda^n(P_n)\\
    &=\int_{A(\prod_n)}\chi_{P_n}\text{d}\lambda^n\\
    \text{change of variables }&=\int_{A(\prod_n)}\abs{\det A}\text{d}\lambda^n\\
    &=\abs{\det A}\\
    \text{by the prop of det }&=\sqrt{\det A^TA}
\end{aligned}$$
\section{Prop}
$$vol(P_n)=\sqrt{\det G}$$
\chapter{Unitary Space}
\section{Def}
A complex inner vector space $(H,h)$ where $h$ is hermitian and positive define then it's called \textbf{unitary space}

As in the Euclidean space, we have orthonormal basis and define a norm,then a distance.

$$\norm{x}=\sqrt{h(x,x)}$$
\section{Decomplexification}
Let $V$ be a complex vector space of dimension $n$. We resist the module structure $\mathbb{C}\times V\rightarrow V$ to $\mathbb{R}\times V\rightarrow V$

The $\mathbb{R}$-vector space denoted by $V_\mathbb{R}$ has the same vector of $V$. For any $\mathbb{C}$-linear mapping $f:V\rightarrow W$, the module induces a mapping $$f_\mathbb{R}\rightarrow W_\mathbb{W}$$
\section{Theorem}
Let $V$ be a complex vector of dim $n$
\begin{itemize}
    \item Let $V$ be a complex vector basis of $V$, then $\{v_1,\cdots, v_n,iv_1,\cdots, iv_n\}$ is a real basis of $V_\mathbb{R}$
    \item $f:V\rightarrow W$ is a linear mapping. Assume that it's metric representation with respect to the basis $\{v_1,\cdots, v_n\}$ of $V$ and $\{w_1,\cdots, w_n\}$ of $W$ is $$A=B+iC$$
    where $B,C\in\mathbb{R}$ Then the metric representation of $f_\mathbb{R}$ with respect to the basis $\{v_1,\cdots, v_n,iv_1,\cdots, iv_n\}$ and $\{w_1,\cdots, w_n,iw_1,\cdots, iw_n\}$ is$$\begin{pmatrix}
        B&-C\\C&B
    \end{pmatrix}$$
\end{itemize}
\section{Corollary}
Let $f:V\rightarrow V$ be a $\mathbb{C}$-linear mapping. Then$$\det f_\mathbb{R}=\det f\overline{\det f}$$
\chapter{Complexification}
\section{Def: Complex structure}
Let $W$ be a real vector space of dim $n$ Consider $J:W\rightarrow W$ a linear mapping such that $J^{\circ 2}=-Id$. Then $J$ is called the \textbf{complex structure} of $W$. Then couple $(W,J)$ is a vector space with a complex structure.
\subsection*{Example}$$\begin{aligned}
    J:V_\mathbb{R}&\rightarrow & V_\mathbb{R}\\
    v&\mapsto&iv\\
    iv&\mapsto&-v
\end{aligned}$$
\section{Theorem}
Let $(W,J)$ be a real vector space with a complex structure. Then on $W$ we introduces the following complex module:
$$(a+bi)w:=aw+bJ(w)$$
We obtain a complex vector space $W$ such that $(W)_\mathbb{R}=W$
\section{Corollary}
If $(W,J)$ is a vector space with a complex structure,then $\dim W$ is even. Assume that if it's even, then it's possible to find a basis on $W$ such that $J$ is represented by$$\begin{pmatrix}
    0&-I_n\\I_n&0
\end{pmatrix}$$
Consider a orthonormal basis on $H\{v_1,\cdots, v_n\}$. $G_h:\mathbb{C}^n\rightarrow C^n$ is the Gram matrix of $h$ with respect to $\{v_1,\cdots,v_n\}$ $$G_h=B_iC$$ Now$$G_\mathbb{R}=\begin{pmatrix}
    B&-C\\C&B
\end{pmatrix}$$
Then $G_\mathbb{R}$ defines an inner product and $(H_\mathbb{R},\inprod{}{})$ is Euclidean.
\section{Notation}
Now fix a inner product space. Then $$h(x,y)=a(x,y)+ib(x,y)$$when $a,b: V\times V\rightarrow \mathbb{R}$
\section{Prop}
In the above notation the following structure holds:
\begin{itemize}
    \item [1] $a(x,y)$ and $b(x,y)$ are inner products on $V_\mathbb{R}$, with a symmetric and $b$ skew-symmetric. In addition:$$a(ix,iy)=a(x,y)\quad b(ix,iy)=b(x,y)$$In other word $a,b$ are invariant by the multiplication by $i$ Invariance w.r.t. the complex structure of $H_\mathbb{R}$
    \item [2]The following relations hold. $$a(x,y)=b(ix,y)\quad b(x,y)=-a(ix,y)$$
    \item [3]Any pair of $J$-invariant bilinear forms on $V_\mathbb{R}$ $a,b:V\times V\rightarrow \mathbb{R}$ that are symmetric and symplectic, respectively and s.t. (2) is satisfied. Define an hermitian inner product$$h(x,y):=a(x,y)+ib(x,y)$$ Moreover $h$ is positive define iff $a$ is positive define.
\end{itemize}
\section{Complex Cauchy-Schwartz inequality}
$(H,h)$ is a unitary space with finite dim. Then the inequality$$\abs{h(x,y)}\leq\norm{x}\norm{y}$$holds iff $x$ and $y$ are propositional $(x=ty)$
\section{Corollary:Complex triangle inequality}$$\norm{x+y}\leq\norm{x}+\norm{y}$$
\section{Def:Angle for unitary space}
For $$0\leq\frac{\abs{h(x,y)}}{\norm{x}\cdot\norm{y}}$$
$\exists !\phi\in [0,\frac{\pi}{2}]$$$\cos\phi=\frac{\abs{h(x,y)}}{\norm{x}\cdot\norm{y}}$$
The physical application is that $\phi$ can be considered as probability.
\section{Prop}
Let $(V,g)$ be an inner product space with a non-degenerate inner product hermitian or real symm and $f:V\rightarrow V$ be a linear mapping. Then the following statements are equivalent:
\begin{itemize}
    \item [0] $f$ is isometry
    \item [1] $g(f(x),f(x))=g(x,x)\ \forall x\in V$
    \item [2] Let $\{v_1,\cdots,v_n\}$ be a basis for $V$ and let $G$ be a Gram matrix of $g$ w.r.t. such baisi. If $A$ is the matrix of $f$ w.r.t. $\{v_1,\cdots,v_n\}$, then $$A^TGA=G\text{ or } A^TG\overline A=G$$
    \item [3] $f$ transforms orthonormal basis into orthonormal basis
    \item [4] If the signature of $g$ is $(p,q)\ (\mathbcal{r}_0=0)$, then the matrix of $f$ w.r.t. any orthonormal basis $\{v_1,\cdots,v_p,v_{p+1},\cdots,v_{p+q}\}$ where$$(v_i,v_i)=\begin{cases}
    1&\text{ if }i\leq p\\-1&\text{ if }p<i\leq p+q
    \end{cases}$$
    satisfies the following property:\begin{itemize}
        \item [symm case]$$A^T\begin{pmatrix}
            I_p&0\\0&-I_q
        \end{pmatrix}A=\begin{pmatrix}
            I_p&0\\0&I_q\end{pmatrix}$$
        \item [hermitian case]$$A^T\begin{pmatrix}
            I_p&0\\0&-I_q
        \end{pmatrix}\overline A=\begin{pmatrix}
            I_p&0\\0&I_q\end{pmatrix}$$
    \end{itemize}
\end{itemize}
\chapter{Special operators}
\section{Def}
$(E,\inprod{}{})$ is a Euclidean space, then an isometry $f:E\rightarrow E$ is called an \textbf{orthogonal operator}

$(H,h)$ a unitary space, then an isometry $f:H\rightarrow H$ is said a \textbf{unitary operator}
\section{Corollary}
\label{corollary 63.11}
Orthogonal and unitary operators have the following properties:

w.r.t. an (all) orthonormal basis, they are represented by a matrix $U$
$$UU^T=I_n\text{ or }UU^\dagger=I_n$$
\section{Def: orthogonal matrices}
$$O(n):=\{A\in GL_n(\mathbb{R})\mid AA^T=I_n\}$$
\section{Def: unitary matrices}
$$U(n):=\{A\in GL_n(\mathbb{C})\mid AA^\dagger=I_n\}$$
\subsection*{Remark}
By the corollary \ref{corollary 63.11}, we have\begin{itemize}
    \item $O(n)$ is the set of orthogonal operators of $(\mathbb{R}^n,\inprod{}{})$
    \item $U(n)$ is the set of unitary operators of $(\mathbb{C}^n,\inprod{}{})$
\end{itemize}
\subsection*{Remark}
$$
\begin{aligned}
    O(n)\\\text{isometry}
\end{aligned}
\subseteq
\begin{aligned}
    GL_n(\mathbb{R})
\end{aligned}
\subseteq
\begin{aligned}
    M_{n,n}(\mathbb{R})\\\text{endomorphism}
\end{aligned}$$
Take $T\in O(n),TT^T=I_n$$$(\det T)^2=1\ \Rightarrow\ \det T=\pm 1$$
\section{Notation}
$$\begin{aligned}
    SL_n(\mathbb{R})&:=\{A\in GL_n(\mathbb{R})\mid \det A=1\}\\
    SO(n)&:=\{A\in O_n\mid\det A=1\}
\end{aligned}$$
\chapter{Classification of operators}
$$\begin{aligned}
    &U(1)=\{a\in \mathbb{C}\mid a\overline{a}=1\}=\{e^{i\phi}\mid \phi\in \mathbb{R}\}\\
    &O(1)=\{1,-1\}=U(1)\cap\mathbb{R}
\end{aligned}$$
Let's study $O(2)$ and classify all its elements

$O(n)/SO(n)=\{\pm1\}$ $SO(n)$ is a normal subgroup of $O(n)$ of index 2, namely $$\#(O(n)/SO(n))=2$$
Take $T\in O(2)$, we have two cases:$\begin{cases}
    T\in SO(2)\\T\in O(2)\setminus SO(2)
\end{cases}$
\subsection{$T\in SO(2)$}
Assume $T=\begin{pmatrix}
    a&b\\c&d
\end{pmatrix}$, and $TT^T=Id_2$
$$\begin{cases}
    \det T=ad-bc=1\\a^2+b^2=1\\c^2+d^2=1\\ac+bd=0
\end{cases}$$
This implies $\exists\alpha$ unique op to add by $2k\pi$ s.t.$$a=\cos\alpha\quad b=\sin\alpha$$
We have shown that
$$SO(2)=\left\{\left.\begin{pmatrix}
    \cos\alpha&-\sin\alpha\\\sin\alpha&\cos\alpha
\end{pmatrix}\right| \alpha\in \leftbracket 0,2\pi\leftbracket\right\}$$
Note that $T$ doesn't have eigenvalues for $\alpha\neq 0,\pi$
\subsection{$T\in O(2)\setminus SO(2)$}
$A=\begin{pmatrix}
    1&0\\0&-1
\end{pmatrix}$ is the reflection with respect to the line $y=0$$$TA\in SO(2)$$since $$\det(TA)=\det T\cdot\det A=1$$
By the previous reasoning,$$T=\begin{pmatrix}
    \cos\alpha&-\sin\alpha\\-\sin\alpha&-\cos\alpha
\end{pmatrix}$$
The set $O(2)\setminus SO(2)$ represents reflections.

Consider the mapping$$\begin{aligned}
    U(1) &\stackrel{\cong}\rightarrow&SO(2)\\
    e^{i\phi}&\mapsto&\begin{pmatrix}
        \cos\phi&-\sin\phi\\\sin\phi&\cos\phi
    \end{pmatrix}
\end{aligned}$$
$O(2)\cong U(1)\cup\left(O(2)\setminus SO(2)\right)$
\subsection*{Remark}
Reflections are diagonalizable with eigenvalues $\{\pm 1\}$ and the corresponding eigenvectors are orthogonal.
\section{Theorem}
\begin{itemize}
    \item[1] Let $(H,h)$ be a unitary space. A linear mapping $f:H\rightarrow H$ is unitary iff it's diagonalizable in an orthonormal basis and with eigenvalues in $S^1$
    \item Let $(E,\inprod{}{})$ be a Euclidean space. A linear mapping $f:E\rightarrow E$ is orthogonal iff in some orthonormal basis $f$ is represented by matrix:
    $$\begin{pmatrix}
        R(\phi_1)&&&&&\\
        &\ddots&&&\\
        &&R(\phi_n)&&\\
        &&&Id_1&\\
        &&&&\ddots&\\
        &&&&&Id_m
    \end{pmatrix}$$
    where $$R(\phi_i)=\begin{pmatrix}
        \cos\phi_i&-\sin\phi_i\\\sin\phi_i&\cos\phi_i
    \end{pmatrix}\quad \phi_i\in \leftbracket 0,2\pi\leftbracket$$
    \item[3] The eigenvectors of orthogonal/unitary operators corresponding to different eigenvalues are orthogonal.
\end{itemize}
\chapter{Fourier Coefficient}
\begin{itemize}
    \item[Goal]We have an infinite dim vector space (usually a space of function) we want to express the elements as combinations of "orthogonal" vectors. (w.r.t. some nice inner product)
\end{itemize}
\section{Def: Orthogonal and Orthonormal System}
$V$ is a vector space over $\mathbb{R}$ or $\mathbb{C}$, $\inprod{}{}$ is an inner product which either symm or hermitian.. Moreover, $\inprod{}{}$ is non-degenerate and positive define. 

A set of vectors $\{l_k\mid k\in I\}$ (where $I$ be the set of indexes) is said to be an orthogonal system if$$\inprod{l_j}{l_k}\text{ iff } j\neq k$$
Moreover $\{l_k\}$ is an orthonormal system if$$\inprod{l_j}{l_k}=\delta_{jk}$$
\section{Prop}
Let $\{l_k\}$ be an orthogonal system, then $\{l_k\}$ is a set of non-zero linearly independent vectors.

\section{Prop}
\subsection*{}The inner product $\inprod{}{}$ is continuous (w.r.t. the Euclidean topology in the co-domain, and the product topology on the domain, where on $V$ we put the topology induced by $\inprod{}{}$)
\subsection*{}If $\{f_k\}$ is orthogonal system and $x\in V=\sum\limits_{k=1}^\infty x_kl_k$, then $\forall y\in V$$$\inprod{x}y=\sum\limits_{k=1}^\infty\inprod{x_k}{y}$$
\subsection*{}If $\{f_k\}$ is orthonormal system and $ x=\sum\limits_{k=1}^\infty x_kl_k$, $y=\sum\limits_{k=1}^\infty y_kl_k$, then $$\inprod{x}{y}=\sum\limits_{k=1}^\infty x_k\overline y_k$$or$$\inprod{x}{y}=\sum\limits_{k=1}^\infty x_ky_k$$

\section{Corollary: Pythagoras}
\label{Pythagoras}
\begin{itemize}
    \item [1]If $\{v_k\}$ is an orthogonal system and $v=\sum\limits_{k=1}^\infty v_k$, then$$\norm{v}^2=\sum\limits_i\abs{v_i}^2$$
    \item [2]If $\{l_k\}$ is an orthonormal system and $x=\sum\limits_{k=1}^\infty x_kl_k$, then$$\norm{x}^2=\sum\limits_i\abs{x_i}^2$$
\end{itemize}
\section{Def: Fourier coefficient}
Let $\{l_k\}$ be an orthogonal system in $V$. Assume that $x=\sum\limits_{k=1}^\infty x_kl_k$, then $$x_k:=\frac{\inprod{x}{l_k}}{\inprod{l_k}{l_k}}$$
is called a \textbf{Fourier coefficient} of $x$ in $\{l_k\}$

Consider $x\in V$, then the Fourier series of $x$ in $\{l_k\}$ is $$x\sim\sum\limits_{k=1}^\infty x_k l_k$$
We don't know whether it converges to $x$ yet.
\section{Main example}
$$V=L^2(X,\mathbb{K})/\sim$$
where $X\in \mathbb{K}^n$ a measurable subset (w.r.t. $\lambda^n$), $\mathbb{K}=\mathbb{R}\text{ or }=\mathbb{C}$
$$L^2(X,\mathbb{K}):=\{f:X\rightarrow \mathbb{K}\mid \int_X\abs{f}^2\text{d}x<+\infty\}$$
We define a equivalence relation on $L^2(X,\mathbb{K})$ by$$f\sim g\text{ iff }\lambda^n\left(\{x\in X\mid f(x)\neq g(x)\}\right)=0$$
we identify two functions equal if they're equal on almost everywhere(only diff on set that measures zero) From now on, we write elements in $V$ simply by representatives.

We define an inner product on $V$:
$$\begin{aligned}
    \inprod{}{}:V\times V &\rightarrow &\mathbb{K}\\
    (f,g)&\mapsto&\int_Xf\overline g\text{d}\lambda^n
\end{aligned}$$
(Well defined w.r.t. $\sim$ ?) Check that is $\int_Xf\overline{g}\text{d}\lambda^n$ well defined? Recall that$$\norm{f(x)\overline{g(x)}}=\norm{f(x)}\norm{g(x)}$$ Then inequality:
$$\begin{aligned}
    &\norm{f(x)}\norm{g(x)}&\leq\frac{1}{2}(\norm{f(x)}^2+\norm{g(x)}^2)\\
    \Leftrightarrow&0&\leq(\norm{f(x)}+\norm{g(x)})^2
\end{aligned}$$
We always have $\norm{f(x)}\norm{g(x)}\leq\frac{1}{2}(\norm{f(x)}^2+\norm{g(x)}^2)$ It follows that $\inprod{f}{g}$ is well-defined. It easy to show that $\inprod{}{}$ is hermitian. The only non-trivial thing is:
$$0=\inprod{f}{f}\Leftrightarrow f(x)=0\text{ almost everywhere}$$
This means that if we don't use $\sim$ that we cannot say $\inprod{}{} $is positive defined.

Consider the following Integral:
$$\int_{\pi}^\pi e^{imx}e^{-inx}\text{d}x=\begin{cases}
    0 & \text{ if }m\neq n\\2\pi&\text{ if }m=n
\end{cases}$$
So $\{x\mapsto e^{imx}\mid m\in \mathbb{Z}\}$ is an orthogonal system for $V=L^2([-\pi,\pi],\mathbb{C})/\sim$. To make it orthonormal, consider
$$\left\{\left.\frac{1}{\sqrt{2\pi}}e^{imx}\right| n\in \mathbb{Z}\right\}$$
If you want to replace $[-\pi,\pi]$ by $[-a,a]$, consider:
$$\left\{\left.\frac{1}{\sqrt{2\pi}}e^{\frac{imx}a}\right| n\in \mathbb{Z}\right\}$$
This is an orthonormal system for $L^2([-a,a],\mathbb{C})/\sim$ In the real case, consider the following integrals:
$$\int_{-\pi}^\pi\cos(mx)\cos(nx)\text{d}x=\begin{cases}
    0 & \text{ if }m\neq n\\\pi &\text{ if }m=n\neq0\\2\pi&\text{ if }m=n=0
\end{cases}$$
$$\int_{-\pi}^\pi\cos(nx)\sin(nx)\text{d}x=0$$
$$\int_{-\pi}^\pi\sin(nx)\sin(mx)\text{d}x=\begin{cases}
    0 & \text{ if }m\neq n\text{ or }mn=0\\\pi &\text{ if }m=n\neq0
\end{cases}$$
If follows that $\{1,\cos(nx),\sin(mx)\mid(m,n)\in \mathbb{N}^2\}$ is an orthogonal system for $L^2([-\pi,\pi],\mathbb{R})$
$$f\sim\frac{a_0}{2}+\sum\limits_{k=1}^\infty a_k\cos(kx)+b_k(f)\sin(kx)$$
$$\begin{cases}
    a_k=\frac{1}\pi\int_{-\pi}^\pi f(x)\cos(kx)\text{d}x\\
    b_k=\frac{1}\pi\int_{-\pi}^\pi f(x)\sin(kx)\text{d}x
\end{cases}$$
For instance if $f=Id$ then $a_k=0,b_k=(-1)^{k+1}\frac{2}{k}$
\chapter{Convergence}
Always assume that the orthogonal system countable this chapter
\section{Prop}Take $x\in V$ and let$$\overline x=\sum\limits_{k=1}^\infty\frac{\inprod{x}{l_k}}{\inprod{l_x}{l_x}}l_k$$
Then if we write $x=\overline x+h$ then $h$ is orthogonal to $\overline{x}$ and $h$ is orthogonal to the topological closure of $\left<\{l_k\}\right>$
\subsection*{Remark}
By Pythagoras Theorem \ref{Pythagoras}, since $x=\overline{x}+h$
$$\norm{x}^2=\norm{\overline{x}}^2+\norm{h}^2\geq\norm{\overline{x}}^2$$
If we write that inequality with respect to the Fourier coefficients, we get \textbf{Bessel's inequality}\\Note that
$$\norm{x}^2=\sum\limits_k^\infty\abs{\frac{\inprod{x}{l_m}}{\inprod{l_k}{l_k}}}\inprod{l_k}{l_k}=\sum\limits_k\frac{\abs{\inprod{x}{l_k}}}{\inprod{l_k}{l_k}}$$
$$\sum\limits_{k}^\infty\frac{\inprod{x}{l_k}}{\inprod{l_k}{l_k}}\leq\norm{x}^2\quad(\text{Bessel's inequality})
$$
So far we have assumed that the Fourier series converges to prove Bessel's inequality. But we DON'T NEED this assumption
\section{Theorem}
Assume $\{l_k\}$ is orthonormal. Let $x_k=\inprod{x}{l_k}$. If $V$ is complete. then $\sum\limits_{k}x_kl_k$ converges.
\subsection*{Remark}In the proof we assumed $\{l_k\}$ orthogonal. But this is not essential.
\textbf{We have studied the existence of the limit} $x$ \textbf{. What about the relation between } $\overline{x}$\textbf{ and }$x$
\section{Prop}\label{Prop 67.3}
Let $\{l_k\}$ be an orthogonal system. Take $x\in V$ and assume that
$$V\ni\overline{x}=\sum\limits_{k=1}^\infty
\frac{\inprod{x}{l_k}{\inprod{l_x}{l_x}}}l_k$$
Then for any $y=\sum\limits_{k}d_kl_k\ (d_k\in \mathbb{F})$ it holds that:
$$\norm{x-\overline{x}}\leq\norm{x-y}$$
The equality is true iff $\overline{x}=y$
\section{Def}
A family of vectors $\mathcal{F}=\{x_\alpha\mid\alpha\in A\}$ in a normed vector space $V$ is \textbf{complete} in a subset $E\subseteq V$ if every vector $x\in E$ can be approximated with arbitrary accuracy by a \textbf{finite} linear combination of elements in $\mathcal{F}$
\subsection*{Another statement}
Let $L=\{\mathcal{F}\}$, then $\mathcal{F}$ is complete in $E$ if $E\subseteq\overline{L}$
\section{Weierstrass Approximation Theorem}
Let $f\in \mathcal{C}([a,b])$. For any $\epsilon>0$, there exists a polynomial $p\in \mathbb{F}[x]$ such that for any $x\in [a,b]$, we have
$$\abs{f(x)-p(x)}<\epsilon$$
In fact$$\norm{f-p}=\sqrt{\int_a^b\abs{f-p}^2\text{d}\lambda}<\epsilon\sqrt{b-a}$$
\section{Prop}
\label{Prop 63.1}
Let $V$ be a complete vector space over $\mathbb{F}$ with inner product $\inprod{}{}$ hermitian or real symm, and positive define and non-degenerate.

Moreover, $\{l_k\}$ is an orthogonal system at most countable. Then the following conditions are equivalent:
\begin{itemize}
    \item [1]$\{l_k\}$ is complete in $E\subseteq V$
    \item [2]For any $x\in E$, we have $x=\sum\limits_{k}\frac{\abs{\inprod{x}{l_k}}}{\inprod{l_k}{l_k}}l_k$
    \item [3]Any vector $x\in E$ satisfies$$\norm{x}^2=\sum\limits_{k}\frac{\abs{\inprod{x}{l_k}}^2}{\inprod{l_k}{l_k}}$$
\end{itemize}
\section{Def: Hamal basis}A countable family of vectors $\{b_k\}$ is a \textbf{Hamal basis} of $V$ if any $v\in V$ there exists a unique sequence $\{\alpha_k\}$ in $\mathbb{K}$ with $\alpha_k=0$ for all but finitely many $k$ s.t. $$v=\sum\limits_k\alpha_kb_k$$
(In this def we don't need to use the topological properties of $V$)
\section{Def: Schauder basis}A countable family of vectors $\{b_k\}$ is a \textbf{Schauder basis} for $V$ if for any $v\in V$ there exists a unique sequence $\{\alpha_k\}$ such that$$v=\sum\limits_k a_kb_k\quad \text{(as convergent series)}$$

A Hamal basis is a Schauder basis(? to prove an element in basis can't be represented by others). In particular, a Schauder basis is a complete family of vectors (in $E=V$)

In pervious, we've proved that if $\{l_k\}$ is an orthogonal complete system (in $E=V$) with $V$ complete, then any $x\in V$ can be written as$$x=\sum\limits_k\alpha_kl_k$$ when $\alpha_k$ are the Fourier coefficients.

In general it's FALSE that a complete family $\{b_k\}$ is a Schauder ($x\in \overline{\left<\{b_k\}\right>}$)
\section{Important Result}
\subsection*{1}
$L^2(\leftbracket -\pi,\pi\leftbracket,\mathbb{K})$ is complete as topological vector space.
\subsection*{2}$\{1,\cos kx,\sin kx\mid k\in \mathbb{N}_{\geq 1}\}$ is a complete family.
\section{Def}
$f:X\setminus\{x_0\}\rightarrow\leftbracket 0,+\infty\leftbracket$ we say that $f$ is \textbf{unbounded} at $x_0$ if $\forall U\ni x_0,\ M>0$$$ \exists x\in U \text{ s.t. }f(x)>M$$
\section{Def: extend by periodicity}
Let $f:\leftbracket -\pi,\pi\leftbracket\rightarrow \mathbb{R}$ extend this func by periodicity.
$$\tilde{f}=f(x-2k\pi)\quad k\in\mathbb{Z}$$

\section{Theorem}
\label{Lp complete}
$L^p(X,\mu)$ is complete w.r.t the topology induced by $\norm\cdot_p$
\section{Def: Dirichlet kernel}$$D_n(x):=\sum\limits_{k=-n}^ne^{iku}=\frac{\sin(n+\frac{u}2)}{\sin\frac{u}2}$$
this is called \textbf{Dirichlet kernel}, which has the prop
$$\frac{1}{2\pi}\int_{-\pi}^\pi D_n(u)\text{d}u=\frac{1}{\pi}\int_0^\pi D_n(u)\text{d}u=1$$
Back to $T_n$ putting $u=x-t$
$$\begin{aligned}
    T_n(x)&=\frac{1}{2\pi}\int_{-\pi}^\pi f(x-u)D_n(u)\text{d}u\\
    &=\frac{1}{2\pi}\int_{-\pi}^\pi f(x-u)\frac{\sin(n+\frac{u}2)}{\sin\frac{u}2}\text{d}u
\end{aligned}$$
Now use that $D_n$ is an even function
$$T_n(x)=\frac{1}{2\pi}\int_0^\pi \left(f(x-u)+f(x+u)\right)D_n(u)\text{d}u=\frac{1}{2\pi}\int_0^\pi\left(f(x-u)+f(x+u)\right)\frac{\sin(n+\frac{u}2)}{\sin\frac{u}2}\text{d}u$$
\section{Riemann-Lebesgue's Lemma}
\label{Riemann-Lebesgue's Lemma}
Let $f::[a,b]\rightarrow\mathbb{R}$ be an integrable function. Then $$\lim\limits_{\lambda\rightarrow+\infty}\int_a^bf(x)e^{i\lambda x}\text{d}x=0$$
\section{Corollary}
$$\lim\limits_{\lambda\rightarrow+\infty}\int_a^bf(x)\cos\lambda x\text{d}x=0$$
$$\lim\limits_{\lambda\rightarrow+\infty}\int_a^bf(x)\sin\lambda x\text{d}x=0$$
\section{Localization Principle}\label{Localization Principle}
Let $f,g\in L^2([-\pi,\pi],\mathbb{K})$. If $f,g$ coincide in a neighborhood of $x_0\in \rightbracket -\pi,\pi\leftbracket\ (f=g)$, the Fourier series $$f\sim\sum\limits_{-\infty}^{+\infty}c_k(f)e^{i\lambda x}\quad g\sim\sum\limits_{-\infty}^{+\infty} c_k(g)e^{i\lambda x}$$
\textit{either} both diverges \textit{or} both converges. Moreover if they converges ar $x_0$, then their limits are the same (NOT to be $f(x_0)=g(x_0)$)

\section{Def: Dini's Condition}\label{Dini's conditions}
Let $U_x^0=\leftbracket-\delta,x\leftbracket\cup\rightbracket x,\delta\leftbracket$ and $f:U_x^0\rightarrow \mathbb{C}$. We say that $f$ satisfies \textbf{Dini's Condition} at $x$ if \begin{itemize}
    \item $f(x_-)$ and $f(x_+)$ exists and finite
    \item $\exists>0$ s.t. $$\int_0^\epsilon\abs{\frac{(f(x-t)-f(x_-))+(f(x+t)-f(x_+))}t}\text{d}t<+\infty$$ 
\end{itemize}
\section{Theorem: pointwise convergence of Fourier series}
\label{Fourier convergence}
Let $f:\mathbb{R}\rightarrow\mathbb{C}$ be a periodic function of period $2\pi$, such that $f$ is integrable in $[-\pi,\pi]$. If $f$ satisfies the Dini's condition at $x\in \mathbb{R}$, then its Fourier series converges at $x$ and $$\sum\limits_{-\infty}^{+\infty}c_k(f)e^{i\lambda x}=\frac{f(x_-)+f(x_+)}{2}$$
\section{Lemma}
$$\sum\limits_{k=0}^n\sin(k+\frac12)t=\frac{\sin^2\left(\frac{n+1}2\right)}{\sin\frac{t}2}$$
$$F_n(t)=\frac{\sin^2\frac{n+1}2t}{(n+1)\sin^2\frac{t}2}$$
What happens when $t=2k\pi$? Use Tayor's expansion:
$$F_n(t)=\frac{\left(\frac{(n+1)}2t+o(t)\right)^2}{(n+1)\left(\frac{t}2+o(t)\right)^2}\quad t\rightarrow 0$$
$$F_n(t)=n+1$$
Sso we can extend $F_n$ at all points $2l\pi$ by putting $F_n(2k\pi)=n+1$
\section{Def: approximated identity(delta function)}
A family of functions $\{K_n\}_{n\in\mathbb{N}}$ with $K_n:\mathbb{R}\rightarrow \mathbb{R}$ is called a \textbf{approximated identify} if
\begin{itemize}
    \item $\frac{1}{2\pi}\int_{-\infty}^\infty K_n(t)\text{d}t=1\quad \forall n\geq 0$
    \item $K_n(t)\geq 0,\forall t\in \mathbb{R},n\geq 0$
    \item For any $\delta>0$$$\lim\limits_{n\rightarrow+\infty}\int_{\abs{t}>\delta}K_n(t)\text{d}t=0$$

\end{itemize}
\section{Prop}
\label{delta under Fourier}
Consider $$\delta_n(x)=\begin{cases}
    \frac{1}{2\pi}F_n(x)&\text{ if }\abs{x}\leq \pi\\0&\text{ otherwise}
\end{cases}$$
Then $\{\delta_n\}$ is an approximated identify
\section{Fejer Theorem}
Let $f:\mathbb{R}\rightarrow\mathbb{C}$ continuous and with period of $2\pi$ and integrable in $[-\pi,\pi]$. Then $\sigma_{f,n}$ converges uniformly to $f$
\section{Weierstrass approximation}
Let $f:[-\pi,\pi]\rightarrow\mathbb{C}$ be a continuous functions s.t. $f(-\pi)=f(\pi)$. Then such function can be approximated uniformly by $\sigma_n$ arbitrarily
\end{document}