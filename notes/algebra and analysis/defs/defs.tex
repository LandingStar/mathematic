\documentclass{article} 
\usepackage{graphicx} % Required for inserting images
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{color}
\usepackage{hyperref}
\usepackage{xypic}
\usepackage{bbm}
\usepackage{dutchcal}
\hypersetup{hidelinks,
	colorlinks=true,
	allcolors=black,
	pdfstartview=Fit,
	breaklinks=true
}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert} 
\newcommand{\leftbracket}{[}
\newcommand{\rightbracket}{]}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\begin{document}
\section{Def: Tensor}
Let $M$ and $N$ be two $R$-modules. Then exists an $R$-module denoted by $M\otimes _RN$ and a bilinear mapping 
$$t:M\times N\rightarrow M\otimes_RN$$ having the following properties:
\begin{itemize}
    \item[(1)]For any $R$-module $P$ and any bilinear mapping $s:M\times N\rightarrow P$. There exists a unique linear mapping $f_s:M\otimes_RN\rightarrow P$ such that $s=f_s\circ t$
    $$\xymatrix{
        & M\times N \ar[d]^t \ar[r]^s & P \\
        & M\otimes_RN\ar[ur]_{f_s}&
    } $$
    \item[(2)] If $T,t'$ is another couple that satisfies (1) with $s\mapsto g_s$ then there exists a unique isomorphism $$T\cong M\otimes_RN$$
\end{itemize}

Let $\mathcal{F}$ be the free $R$-module generated by $M\times N$
    $$\mathcal{F}=\{\sum\limits_{finite}a_{ij}(m_i,n_i):a_{ij}\in R,m_i\in M,n_i\in N\}$$
    let $\mathcal{G}$ be the $R$-submodule generated by the elements of the following shape $m,m'\in M\quad n,n'\in N\quad \mathbcal{r}\in R$
    $$\begin{aligned}
        &(m+m',n)-(m,n)-(m',n)\\
        &(m,n+n')-(m,n)-(m,n')\\
        &(\mathbcal{r}m,n)-\mathbcal{r}(m,n)\\
        &(m,\mathbcal{r}n)-\mathbcal{r}(m,n)
    \end{aligned}$$
    $$M\otimes_RN:=\mathcal{F}/\mathcal{G}$$
\section{Def}
$$f_s(\mathcal{G}+(m,n)):=s(m,n)$$
Extend this mapping to linearity. This makes the diagram commutative. It's clearly the unique mapping
\section{Def}
The $R$-module $M\otimes_RN$ constructed above is called the tensor product of $M$ and $N$. An element of $M\otimes_RN$ is called tensor. We denote$$t(m,n):=m\otimes n$$ and any elements of this form is called pure tensor. 
\section{Remark}Pure tensors generate $M\otimes_R N$. In particular any tensor can be written as sum of pure tensors.
\section{tensor product and duality}
\subsection{product}
Let $V_1,\cdots,V_n$ be vector spaces as above. Then $$(V_1^\vee\otimes\cdots\otimes V_n^\vee)\cong(V_1\otimes\cdots\otimes V_n)^\vee$$
\subsection{duality}
Let $V$ and $W$ be vector spaces of finite dimension. Then 
$$\mathscr{L}(V,W)\cong V^\vee\otimes W^\vee$$
\section{Def}
We went to define the tensor product of linear mappings. let$M_1,M_2,N_1,N_2$ be $R$-modules and let $f_i:M_i\rightarrow N_i$ be linear mappings. Then we define $$
\begin{aligned}
    f_1\otimes f_2:M_1\otimes M_2 &\rightarrow N_1\otimes N_2\\
    m_1\otimes m_2 &\mapsto f_1(m_1)\otimes f_2(m_2)
\end{aligned}
$$
This is a linear mapping
$$\xymatrix{
    M_1\times M_2\ar[d]\ar[r]^{f_1\times f_2}& N_1\times N_2\ar[d]\\
    M_1\otimes M_2 \ar@{-->}^{f_1\otimes f_2}[r] &N_1\otimes N_2
}$$
\section{Extension of scalars}
Let $\varphi:R\rightarrow S$ be a commutative unitary ring homomorphism. Let $M$ be a $R$-module. Goal is to give to $M$ also a structure of $S$-module "conveyed by $\varphi$"

Note that $S$ has a structure of $R$-module $s\in S,\mathbcal{r}\in R$$$\mathbcal{r}s:=\varphi(\mathbcal{r})s$$
Now take thw tensor product $M\otimes_RS$. Now we give a structure of $S$-module to $M\otimes_RS$. 

Take $s\in S$$$s(\underbrace{m\otimes s'}\limits_{\in M\otimes_RS}):=m\otimes ss'$$
note that $ss'$ is a multi in $S$ and we cannot product $sm$.

Notice we've a mapping
$$\begin{aligned}
    i: M&\rightarrow M\otimes_RS\\
    m &\mapsto m\otimes s
\end{aligned}$$
Be careful, in general the mapping $i$ is NOT injective.
\section{Prop}
Let $K\subseteq L$ be a field extension and let $V$ be a $K$-vector space. Moreover let's denote $V_L=V\otimes_KL$. If $\{e_i\}_{i=1}^n$ is a basis of $V$ then $\{e_i\otimes 1\}_{i=1}^n$ is a L-basis of $V_L$.($V_L$ has the same dim of $V$)
\section{Def}
We denote
$$
\begin{aligned}
    T_p^q:=&(V^\vee)^{\otimes p}\otimes V^{\otimes q}\qquad p,q\in \mathbb{N}\\
    =&\underbrace{V^\vee\otimes\cdots\otimes V^\vee}\limits_{p\text{ times}}\otimes \underbrace{V\otimes\cdots\otimes V}\limits_{q\text{ times}}\\
\end{aligned}
$$
An element of $T_p^q(V)$ is called a tensor of type $(p,q)$ (or a mixed tensor which is $p$-covariant and $q$-contravariant)

Let's denote:
$$T(V):=\bigoplus\limits_{q\in \mathbb{N}} T_0^q(V)$$
On $T(V)$ we have following operation:
$$
\begin{aligned}
    T_0^l(V)\times T_0^q(V)&\rightarrow T_0^{l+q}(V)\\
    ((x_1\otimes\cdots\otimes x_l),(y_1\otimes\cdots\otimes y_q))&\mapsto x_1\otimes\cdots\otimes x_l\otimes y_1\otimes\cdots\otimes y_q
\end{aligned}
$$

With this operation $T(V)$ becomes a $K$-algebra. It called the tensor algebra associated to $V$

\section{Def}
The quotient algebra $$\bigwedge(V):=T(V)/\left\{\sum\limits_{i(finite)}(y_1\otimes\cdots\otimes y_{m_i})\otimes(x_i\otimes x_i)\otimes(z_1\otimes\cdots\otimes z_{n_i})\right\}$$
is a K-algebra, which called the exterior algebra of V
$$
\begin{aligned}
    \pi: &T(V) &\rightarrow \bigwedge(V)\\
    &x_1\otimes\cdots\otimes x_n &\mapsto x_1\wedge\cdots\wedge x_n
\end{aligned}$$
\section{Notation}
$$\bigwedge(V)=\bigoplus \limits_{n\in\mathbb{N}} \bigwedge^n(V)$$
$$\bigwedge^n(V):=T^n_0(V)/(W\cap T_0^n(V))$$
this is called $n$-fold exterior product
\section{Prop}
\label{VIII-46.8}
FIx a vct space V. For any alternating multi-linear mapping
$$s:\underbrace{V\times\cdots\times V}\limits_{n\text{ times}}\rightarrow W$$
when W is another vct space, there exists a unique linear mapping$$g_s:\bigwedge\limits^n(V)\rightarrow W$$
such that the following diagram commutes
$$\xymatrix{
    & V^n\ar[d]_t\ar[r]^s & W\\
    &T^n_0(V)\ar@{-->}[ur]^{f_s}\ar[d] &\\
    & \bigwedge\limits^n(V)\ar[uur]_{g_s}
}$$
\section{Prop}
Let V be a vct space of dimension n with a basis $\{e_1,\cdots,e_n\}$. Then $\bigwedge\limits^k(V)$ is a vct space with a basis given by $$\mathcal{B}=\{e_{i_1}\wedge\cdots\wedge e_{i_k}\big|\ 1\leq i_1<\cdots<i_k\leq n\}$$
In particular, $\bigwedge\limits^k(V)$ has dimension $\left(\begin{aligned}
    &n \\ &k
\end{aligned}
\right)$
\section{Def}
Let V be a vct space of dimension n, then 
$$\det(V)=\bigwedge\limits^n(V)$$
is called the determinant of V. It is a vct space of dimension $1=\binom{n}{n}$ and a basis is given by $$e_1\wedge\cdots\wedge e_n$$
when $\{e_1,\cdots,e_n\}$ is a basis of V.
\section{Def}
$$
\begin{aligned}
    g_{\widetilde{f}}=\bigwedge\limits^kf: &\bigwedge\limits^k(V) &\rightarrow &\bigwedge\limits^k(V)\\
    &v_1\wedge\cdots\wedge v_k &\mapsto & f(v_1)\wedge\cdots\wedge f(v_n)
\end{aligned}$$
\section{Def}
Let $F:V\rightarrow V$ be a linear mapping. A subspace $V_0\subseteq V$ is said to be an invariant subspace of $F$ is $F(V_0)\subseteq V_0$
\section{Def}
A linear mapping $f:V\rightarrow V$ (finite dim) is diagonalizable if the following equivalent conditions are satisfied
\begin{itemize}
    \item [1] $V$ decomposes as a direct sum of one-dimensional invariant subspace of $f$
    \item [2] There exists a basis of $V$, in which the matrix $A_f$ is diagonal.
\end{itemize}
\section{Def}
V a vector space over K $dim(V)=n,f\in \mathscr{L}(V;V)$ let $A_f$ be an associated matrix (in any basis) the mapping
$$\begin{aligned}
    P: &K &\rightarrow& K\\&t&\mapsto& \det(tI_n-A_f)
\end{aligned}$$
This is a polynomial in $K[t]$ (with degree $n$)
\section{Def}
Let $a_0+a_1t+\cdots+a_nt^n= Q(t)\in K[t]$, then for $f\in \mathscr{L}(V;V)$ we define 
$$Q(f):=a_0id_V+a_1f+a_2f^{\circ 2}+\cdots+a_nf^{\circ n}$$
\begin{itemize}
    \item[Remark]
From now on we write
$$f^{\circ k}=f^k$$
\end{itemize}
these are operations in $\mathscr{L}(V;V),+,\circ$

we say that $Q$ annihilates $f$ if $Q(f)=0$
\section{Prop}
\label{Prop.48.17}
Let $f\in \mathscr{L}(V;V)$. There exists a polynomial $Q\in K[t]\setminus\{0\}$ that annihilates $f$ (i.e. $Q(f)=0$)
\subsection*{Remark}The proof of this proposition also gives the degree of a polynomial that annihilates $(\leq n^2)$
\section{Def}
Let $m(t)\in K[t]\setminus\{0\}$ be a monic polynomial of minimal degree that annihilates $f\in \mathscr{L}(V;V)$. Then $m(t)$ is called minimal polynomial of $f$
And by prop above (\ref{Prop.48.17}), $m(t)$ exists.
\section{Prop}
If $m(t)$ is minimal polynomial of $f$, then $m(t)$ is unique.
\section{Prop}
Let $Q\in K[t]\setminus\{0\}$ be a polynomial that annihilates $f$. Then $m_f\mid Q$ 
\section{Theorem: Cayley-Hamilton Theorem}
The characteristic polynomial $P_f$ annihilates $f$
\section{Theorem}Let $f\in\mathscr{L}(V;V)$ when $V$ is a vector space of dim $n$, over an algebraically closed field.\\Then
\begin{itemize}
    \item [(1)]$f$ can be represented by a Jordan matrix
    \item [(2)]This above matrix is unique up to permutation of the Jordan blocks
\end{itemize}
\section{Def}
Let $f\in\mathscr{L}(V;V)$ and let $\lambda\in K$. A vector $w\in V\setminus\{0\}$ is called a root vector of $f$ corresponding to $\lambda$, if there exists $\mathbcal{r}\in\mathbb{N}$ s.t. $$(f-\lambda id_V)^\mathbcal{r}(w)=0$$
\subsection*{Remark}
Eigenvector are root vectors (corresponding to their eigenvalues) take $\mathbcal{r}=1$

\subsection*{Remark}
Let $J_\mathbcal{r}(\lambda)$ be a Jordan block. Then any $\sigma\in V$ is a root vector of $f$ corresponding to $\lambda$. In fact:
$$(J_\mathbcal{r}(\lambda)-\lambda I_n)^m=0\quad\text{if }m\geq\mathbcal{r}$$
\section{Prop}
\label{Prop 48.27}
Let $K$ be an algebraically closed field. Let $\lambda_1,\cdots,\lambda_k$ be all of distinct eigenvalues of $f(k\geq1)$, then 
$$V=\bigoplus\limits_{i=1}^kV(\lambda_i)$$
\section{Def}
Let $f\in  \mathscr{L}(V;V)$. Then $f$ is said to be nilpotent if there exists $t\in \mathbb{N}$ that $f^t=0$
\section{Lemma}
Let $f$ be a nilpotent mapping, then $Ker (f)\neq\{0\}$
\subsection*{Proof}
Let $\mathbcal{r}$ be the minimal integer s.t. $f^\mathbcal{r}=0$ then
$$f^{\mathbcal{r}-1}(V)\subseteq Ker(f)$$
but $f^{\mathbcal{r}-1}(V)\neq\{0\}$ because of the minimality of $\mathbcal{r}$
\section{Theorem}
Let $f\in\mathscr{L}(V;V)$ be a nilpotent mapping, then there exists a Jordan basis for $f$ that gives a Jordan matrix made of blocks of the type $J_\mathbcal{r}(0)$
\section{Theorem}
Let $K$ be an algebraically closed field. Let $f\in \mathscr{L}(V)$. Then  $f$ admits a Jordan basis (namely there exists a basis s.t. $A_f$ is a Jordan matrix). 
\section{Def}
Let $\lambda$ be an eigenvalue of $f\in \mathscr{L}(V)$
$$E(\lambda):=\ker (f-\lambda Id)$$
This $E(\lambda)$ is called the eigenspace of $\lambda$
$$mult(\lambda)_{geo}=dim(E(\lambda))$$
is called the geometric multiplicity of $\lambda$

Moreover
$$mult(\lambda)_{alg}=\max\left\{k\in \mathbb{N}\left| (t-\lambda)^k\mid P_f(t)\right.\right\}$$
is called the algebraic multiplicity of $\lambda$
\section{Prop}
Let $K$ be algebraically closed. Then $\forall \lambda$ eigenvalues of $f$
$$mult(\lambda)_{geo}\leq mult(\lambda)_{alg}$$
\section{Corollary}
Let $K$ be an algebraically closed field. Let $f\in \mathscr{L}(V)$. $f$ is diagonalizable iff $$\forall\lambda_i\quad mult(\lambda_i)_{geo}=mult(\lambda_i)_{alg}$$
\section{Def}Two matrices $G,G'\in M_{n\times n}(K)$ are said conjugate if $\exists A\in \mathcal{Q}_{n\times n}(K)$ s.t. $G=G'^T$
\section{Def}

Let $p\in \mathbb{R}^n$ be a fixed point
$$\mathbb{R}^n_p:=\{p\}\times\mathbb{R}^n$$
$(p,a)\in \mathbb{R}^n_p,a\in \mathbb{R}^n$
$$\begin{aligned}
    &(p,a)+(p,b)=(p,a+b)\\ &\alpha(p,a)=(p,\alpha a)\  \alpha\in \mathbb{R}
\end{aligned}$$

With these operation $\mathbb{R}^n_p$ is a vector space, which is called the tangent space of $\mathbb{R}^n$ at $p$.

The dual space is $$(\mathbb{R}^n_p)^\vee=\{p\}\times(\mathbb{R}_p^n)^\vee$$
A basis of $\mathbb{R}_p^n$ is denoted by $$(e_1\mid_p,\cdots,e_n\mid_p)$$

$\bigsqcup\limits_p \mathbb{R}^n_p$ is called the tangent bundle of $\mathbb{R}^n$
\subsection{Notation}
$$a\mid_p:=(p,a)$$
\section{Def}

Let $p\in \mathbb{R}^n$ be a fixed point
$$\mathbb{R}^n_p:=\{p\}\times\mathbb{R}^n$$
$(p,a)\in \mathbb{R}^n_p,a\in \mathbb{R}^n$
$$\begin{aligned}
    &(p,a)+(p,b)=(p,a+b)\\ &\alpha(p,a)=(p,\alpha a)\  \alpha\in \mathbb{R}
\end{aligned}$$

With these operation $\mathbb{R}^n_p$ is a vector space, which is called the tangent space of $\mathbb{R}^n$ at $p$.

The dual space is $$(\mathbb{R}^n_p)^\vee=\{p\}\times(\mathbb{R}_p^n)^\vee$$
A basis of $\mathbb{R}_p^n$ is denoted by $$(e_1\mid_p,\cdots,e_n\mid_p)$$

$\bigsqcup\limits_p \mathbb{R}^n_p$ is called the tangent bundle of $\mathbb{R}^n$

We have a projection mapping:
$$ 
\begin{aligned}
    &\bigsqcup \limits_{p}\mathbb{R}^n_p&\stackrel{\pi}{\twoheadrightarrow}&\mathbb{R}^n_p\\ &(p,a)&\mapsto &p
\end{aligned}$$
and 
$$
\begin{aligned}
    \mathbb{R}^n\times\mathbb{R}^n &\cong \bigsqcup \limits_{p}\mathbb{R}^n_p\\ (p,a) &\reflectbox{\ensuremath{\mapsto}} (p,a)
\end{aligned}
$$

Take $\{e_1\mid_p,\cdots,e_n\mid_p\}$ as a basis of $\mathbb{R}^n_p$. The dual basis is denoted by $$\{dx_1\mid_p,\cdots,dx_n\mid_p\}=\{(e_1\mid_p)^\vee,\cdots,(e_n\mid_p)^\vee\}\in (\mathbb{R}_p^n)^\vee$$
$$\begin{aligned}
    dx_i\mid_p: &\mathbb{R}^n_p &\rightarrow &\mathbb{R}\\
    &v=(\sum\alpha_ie_i\mid_p)&\mapsto &\alpha_i
\end{aligned}$$
$$\frac{\partial x_i}{\partial x_j}=dx_i\mid_p(e_j\mid_p)=\left\{\begin{aligned}
    &1&\text{ if }i=j\\
    &0&\text{ if }i\neq j
\end{aligned}\right.$$
Recalled the wedge algebra:
$$\bigwedge(\mathbb{R}^n_p)^\vee:=T(\mathbb{R}^n_p)^\vee/I=\bigoplus\limits_{k\in \mathbb{N}}\bigwedge\limits^k(\mathbb{R}_p^n)^\vee$$
Consider $$\bigwedge\limits^k(\mathbb{R}_p^n)^\vee$$
what's a basis of this vector space?
$$\left\{dx_1\mid_p\wedge\cdots\wedge dx_k\mid_p\textbf{\big|}1\leq i_1<\cdots< i_k\leq n\right\}$$
and $$\dim(\bigwedge\limits^k(\mathbb{R}_p^n)^\vee)={n\choose k}$$
Proved.
\section{Do Carmo Differential forms}
\section{Def}
An exterior $k$-form in $\mathbb{R}^n$ is a mapping:
$$\begin{aligned}
    \omega: &\mathbb{R}^n &\rightarrow &\bigsqcup\limits_{p}\bigwedge\limits^k(\mathbb{R}_p^n)^\vee\\
    &p&\mapsto&\omega(p)
\end{aligned}$$
that's a section of the projection $\pi$
$$(\pi\circ \omega=id_\mathbb{R})=(\omega(p)\in \bigwedge\limits^k(\mathbb{R}^n_p)^\vee)$$
$$\omega(p)=\sum\limits_{1\leq i_1<\cdots<i_k\leq n}a_{i_1,\cdots,i_k}(p)dx_{i_1}\mid_p\wedge\cdots\wedge dx_{i_k}\mid_p\in \bigwedge\limits^k(\mathbb{R}_p^n)^\vee$$
Note that
$$\begin{aligned}
    &\bigsqcup\limits_{p}\bigwedge\limits^k(\mathbb{R}_p^n)^\vee&\stackrel{\pi}{\twoheadrightarrow}&\mathbb{R}^n\\
    &f\mid_p&\mapsto&p 
\end{aligned}$$
$$\omega\leftrightarrow\{a_{i_1},\cdots,a_{i_k}\}$$
if all $a_{i_j}$ are of class $C^m(\mathbb{R})$ the $\omega$ is called a $C^m$-differential $k$-form. If $m=+\infty$ $omega$ is called a smooth $k$-form.
\section{Notation}
$$\omega=\sum\limits_{I}a_Idx_I$$
where $I=(i_1,\cdots,i_k)$
\section{Notation}
When $k=0$ a $0$-form of class $C^m$-differential 0-form is $f\in C^m(\mathbb{R}^n)$$$C^m(\mathbb{R}^n)=\{f:\mathbb{R}^n\rightarrow \mathbb{R}\text{ of class }C^m\}$$
\section{Notation}
$$\Omega^k_{(m)}(\mathbb{R}^n):=\{\text{set of }C^m\text{-diff }k\text{-forms}\}$$
$$\Omega^0_{(m)}(\mathbb{R}^n)=C^m(\mathbb{R}^n)$$
$m$ could be omitted if no confusion.
\section{Def}
Now we have $$\Omega(\mathbb{R}^n)=\bigoplus\limits_{k\in \mathbb{N}}\Omega^k(\mathbb{R}^n)$$ a $\mathbb{R}$-algebra with the $\wedge$-product

And it's also a $\Omega^0(\mathbb{R}^n)$ module and $\Omega^0(\mathbb{R}^n)$-algebra

\section{Def: Pullback of forms}
Let $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ be a mapping of $C^\mathbcal{r}$, then it induces a mapping 
$$\begin{aligned}
    f^*:\Omega^k_{(\mathbcal{r})}(\mathbb{R}^m)&\rightarrow\Omega^k_{(\mathbcal{r})}(\mathbb{R}^n)\\\omega&\mapsto f^*\omega
\end{aligned}$$
and $$f^*(\omega)(p)(v_1,\cdots,v_k)=\omega(f(p))(df\mid_p(v_1),\cdots,df\mid_p(v_k))$$
recalling$$df\mid_p:\mathbb{R}^n_p\rightarrow\mathbb{R}^m_{f(p)}\ \Rightarrow\ df\mid_p(v_i)\in \mathbb{R}^n_{f(p)}$$
\section{Remark}
$f\in \Omega^0(\mathbb{R}^n),\omega\in \Omega^k(\mathbb{R}^n)$$$f\wedge\omega=f\omega$$
\section{Prop}
Let $f:\mathbb{R}^n\rightarrow\mathbb{R}$ be a differentiable mapping. Then
\begin{itemize}
    \item [(1)]for any two forms in $\mathbb{R}^m$$$f^*(\omega\wedge\eta)=(f^*\omega)\wedge(f^*(\eta))$$
    \item [(2)]for $g:\mathbb{R}^p\rightarrow\mathbb{R}^n$ differentiable$$(f\circ g)^*\omega=g^*(f^*\omega)$$
\end{itemize}
\section{Def: Path integral}
\label{Def 52.2}
Let $\gamma$ and $\omega$ be as above.
$$\int_\gamma\omega:=\sum\limits_i\int_{t_k}^{t_{k+1}}\gamma_j^*\omega$$
this is the integral of $\omega$ along the parametric curve $\gamma$ with
$$\gamma=t\mapsto(x_1(t),\cdots,x_n(t))$$
where $x_i(t)=\frac{\text{d}x_i}{\text{d}t}$
\section{Def($\sigma$-finite)}
Let $(X,\Sigma_X,\mu)$ be a measure space. WE say that it's $\sigma$-finite if there exists a sequence $\{E_n\}_{n\in \mathbb{N}}$ of measurable sets. (namely $E_n\in \Sigma_X$) such that $$X=\bigcup\limits_{n\in \mathbb{N}}E_n\text{ and }\mu(E_n)<+\infty, \forall n\in \mathbb{N}$$
\section{Notation}
Take sets $A\subseteq X\times Y$ For $x\in X$, we define
$$A_x:=\{u\in Y\mid(x,y)\in A\}$$
called a \textbf{vertical section} of $A$ or $x-$fiber of $A$

For $y\in Y$ we define
$$A_y:=\{x\in X\mid(x,y)\in A\}$$
called a \textbf{horizontal section} of $A$, or $y$-fiber of $A$
\section{Def}
Let $X$ be a set. then $\mathscr{D}\subseteq\wp(X)$ is a \textbf{Dynkin system} if 
\begin{itemize}
    \item $X\in \mathscr{D}$ and $\varnothing\in \mathscr{D}$
    \item $\forall D\in \mathscr{D}\quad X\setminus D\in \mathscr{D}$
    \item If $\{D_n\}_{n\in \mathbb{N}}$ is a sequence in $\mathscr{D}$ of pairwise disjoint sets, then $$\bigsqcup\limits_{n\in \mathbb{N}}D_n\in \mathscr{D}$$
\end{itemize}
\section*{Remark}
A $\sigma$-algebra is a Dynkin system
\section{Def}
Let $(\mathcal{G}\subseteq\wp(X))$ then $\delta(\mathcal{G})\subseteq\wp(X)$ is called the Dynkin system generated by $\mathcal{G}$ if \begin{itemize}
    \item $\mathcal{G}\subseteq\delta(\mathcal{G})$
    \item If $\mathscr{D}$ is a Dynkin system containing $\mathcal{G}$, then $\delta(\mathcal{G})\subseteq\mathscr{D}$
\end{itemize}
\section{Prop}
If $\mathscr{D}$ is a Dynkin system closed under the intersection, then it's a $\sigma$-algebra, namely
$$\forall(D,E)\in \mathscr{D}^2, D\cap E\in \mathscr{D}\ \Rightarrow\ \forall\{D_n\}_{n\in \mathbb{N}}\in \mathscr{D}^\mathbb{N}\quad \bigcup\limits_{n\in \mathbb{N}}D_n\in \mathscr{D}$$
\section{Prop}
\label{Prop 53.7}
Let $X$ be a set and let $\mathcal{G}\subseteq\wp(X)$. Assume that $\mathcal{G}$ is closed under the finite intersection. Then $$\delta(\mathcal{G})\subseteq\sigma(\mathcal{G})$$
\section{Theorem}
Let $(X,\Sigma_X,\mu)$ and $(Y,\Sigma_Y,\nu)$ be $\sigma$-finite measure spaces. Then $\forall E\in \Sigma_X\otimes\Sigma_Y$, the functions
$$\begin{aligned}
    f_E: &X &\rightarrow &\mathbb{R}\cup\{+\infty\}\\ &x &\mapsto &\nu(E_x)\\
    g_E: &Y &\rightarrow &\mathbb{R}\cup\{+\infty\}\\ &y &\mapsto &\mu(E_y)
\end{aligned}$$
are respectively $\Sigma_X$-measurable and $\Sigma_Y$-measurable
\section{Theorem}
Let $(X,\Sigma_X,\mu)$ and $(Y,\Sigma_Y,\nu)$ be $\sigma$-finite measure spaces. There exists a unique $\sigma$-finite measure $\mu\times\nu$ on $(X\times Y,\Sigma_X\otimes\Sigma_Y)$ such that
$$\mu\times\nu(S_1\times S_2)=\mu(S_1)\nu(S_2)\quad \forall(S_1,S_2)\in\Sigma_X\times \Sigma_Y$$
and moreover, we have 
$$(\mu\times\nu)(E)=\int_Xf_E\text{d}\mu=\int_Yg_E\text{d}\nu$$
\section{Def: Push-forward measure}
Let $(X,\Sigma_X,\mu)$ be a measure space, and let $(Y,\Sigma_Y)$ be a measurable space. If $f:X\rightarrow Y$ is a measurable function, then define:
$$f_{*\mu}(E)=\mu(f^{-1}(E))\quad \forall E\in \Sigma^Y$$
This is a measure on $Y$, called the push forward of $\mu$ through $f$
\section{Fubini-Tobelli Theorem}
Let $(X,\Sigma_X,\mu)$ and $(Y,\Sigma_Y,\nu)$ be two $\sigma$-finite measure spaces. Let $(X\times Y,\Sigma_X\otimes\Sigma_Y,\mu\times\nu)$ be the product space. Let $f:X\times Y\rightarrow \mathbb{R}\cup\{-\infty,+\infty\}$ be a measurable function. Then 
$$\begin{aligned}
    \int_{X\times Y}\abs{f}\text{d}(\mu\times\nu) &=\int_X(\int_Y\abs{f(x,y)}\text{d}\nu(y))\text{d}\mu(x)\\
    &=\int_X(\int_Y\abs{f(x,y)}\text{d}\nu(y))\text{d}\mu(x)
\end{aligned}$$
\section{Notation}
For any mapping $\gamma:[a,b]\rightarrow U$
\begin{itemize}
    \item $\gamma$ is called a closed curve if $\gamma(a)=\gamma(b)$ and $\gamma$ is a curve
    \item $\gamma$ is called a path if $\gamma$ is of class $C^0$
    \item $\gamma$ is called a loop if $\gamma$ is a closed path
\end{itemize}
\section{Def: Lebesgue number}
Let $(X,\rho)$ be a metric space and $\mathcal{U}=\{U_i\}$ be an open covering $X$

A \textbf{Lebesgue number} $\delta=\delta_{\mathcal{U}}$ (of the open covering $\mathcal{U}$) is a non-negative number that:

If $Z\subseteq X$ is a subset with $diam(Z)<\delta$, then $Z\subseteq U_j$ for some $U_j\in \mathcal{U}$
\subsection*{Remark}\begin{itemize}
    \item $\delta'<\delta$ is also a Lebesgue number
    \item In principle, a Lebesgue number $\delta$ can be 0
\end{itemize}
\section{Lemma} If $X$ is compact, then for any open covering there exists a positive Lebesgue number.
\section{Theorem(homotopy invariance of the integrals)}
Ler $\omega$ be a closed form on an open set $U$. Let $\gamma_0,\gamma_1$ be homotopy paths in $U$, then $$\int_{\gamma_0}\omega=\int_{\gamma_1}\omega$$
\section{Def: Free Homotopy}
Let $\gamma_0,\gamma_1:[a,b]\rightarrow U$ be two loops (namely $\gamma(a)=\gamma(b)$)

A \textbf{free homotopy} between $\gamma_0$ and $\gamma_1$ is a continuous mapping:
$$\begin{aligned}
    H:&[a,b]\times[0,1]&\rightarrow&U\\
    &(s,t)&\mapsto&H(s,t)
\end{aligned}$$
such that\begin{itemize}
    \item $$H(\cdot,0)=\gamma_0\quad H(\cdot,1)=\gamma_1$$
    \item For any fixed $t_0$$$H(\cdot,t_0)$$ is a loop
\end{itemize}
\section{Notation}
A path $\gamma:[a,b]\rightarrow I$ is said simple if $\gamma\mid_{\rightbracket a,b\leftbracket}$ is injective (No self-cross this is)
\section{Jordan Theorem}
Let $\gamma$ be a simple loop $\gamma:[a,b]\rightarrow U$ , then $\mathbb{R}^2\setminus\gamma([a,b])$ consists exactly of two connected components. One of this is bounded (interior), the other one unbounded (exterior). Moreover $\gamma([a,b])$ is the boundary of two components.
\section{Def}
Let $c:[a,b]\rightarrow S^1$ be a closed curve. Let $\varphi$ be the angular function of $c$. We define the winding number of $c$ as:
$$n(c)=\frac{1}{2\pi}(\varphi(b)-\varphi(a))$$
Since $c$ us a closed curve, $n(c)\in \mathbb{Z}$
\section{Def}
Let $\gamma:[a,b]\rightarrow\mathbb{R}^2\setminus\{p\}$ be a closed curve. ($\gamma_p+\rho(t)c(t)$), when $c(t)\in S^1$$$\gamma(t)=p+\rho(t)(\cos(\theta(t))+\sin(\theta(t)))$$
Then we define the winding number of $\gamma$ at $p$
$$n_p(\gamma):=n(c)$$
\section{Prop}
Let $\gamma=p+\rho(t)c(t)$ be a closed curve $\gamma:[a,b]\rightarrow \mathbb{R}^2\setminus\{p\}$ then
$$n_p(\gamma)=\frac{1}{2\pi i}\int_C\omega_0$$
where $$\omega_0=-\frac{y}{x^2+y^2}\text{d} x+\frac{x}{x^2+y^2}\text{d}y$$
\section{Prop}
Let $\gamma_0,\gamma_1:[0,b]\rightarrow\mathbb{R}^2\setminus\{p\}$ be two closed curves. Then they're freely homotopic iff$$n_p(\gamma_0)=n_p(\gamma_1)$$
\section{Def}
Let $F:U\subseteq \mathbb{R}^2\rightarrow\mathbb{R}^2$ be a differential mapping. We say that $p\in U$ is a zero of $F$ if $F(p)=0$. If then exists a neighborhood $V$ of $p$ such that $V$ contains no zero of $F$ other then $p$,  then $p$ is called isolated zero.

If $p$ is a zero of $F$ and $\text{d}F\mid_p$ is non singular at $p$, then we say that $p$ is a simple zero.
\section{Def} The index of $F$ in $D$, is defined as
$$n(F,D):=\frac{1}{2\pi}\int_C\theta$$
See that $\theta=F^*\omega_0$, $\omega_0=\frac{-y\text{d}x+x\text{d}y}{x^2+y^2}$
$$\begin{aligned}
    n(F,D)&=\frac{1}{2\pi}\int_C\theta\\
    &=\frac{1}{2\pi}\int_CF^*\omega_0\\
    &=\frac{1}{2\pi}\int_{F\circ}\omega_0\\
    &=(\text{winding number of }F\circ C\text{ at the center of }F D)
\end{aligned}$$ 
\section{Remark}
$$n(F,D)=\frac{1}{2\pi}\int_C\theta=\frac{1}{2\pi}\int_{F\circ C}\omega_0$$
\section{Prop}
If $n(F,D)\neq 0$ then $\exists q\in D$ s.t. $F(q)=0$
\section{Def}
A simple zero $p$ of $F$ is said \textbf{positive} if $\det(\text{d}_p F)>0$, otherwise is said \textbf{negative} ?(what's =0?)
\section{Kronecker Index Theorem}
Assume that $F;U\subseteq \mathbb{R}^2\rightarrow\mathbb{R}^2$ has only finite simple zeros in a disk $D\subseteq U$ and none of them in $\partial D$. Then
$$n(F,D)=P-N$$
where $P$ is the number of positive simple zeros and $N$ is the number of negative simple zeros.
\section{Def}
Let $\mathcal{P}=\{t=t_a,t_1,\cdots,t_n=b\}$, $p_i=\gamma(t_i)$
$$l_{\mathcal{P}}(\gamma)=\sum\limits_{i=0}^n\norm{p_{i+1}-p_i}$$
The length of $\gamma$ is 
$$l(\gamma):=\sup\limits_p\{l_p(\gamma)\}$$
If $l(\gamma)<+\infty$, then path $\gamma$ is said rectifiable.
\section{Prop}
Let $\gamma:[a,b]\rightarrow\mathbb{R}^n$ be of class $C^1$, then $\gamma$ is rectifiable and $$l(\gamma)=\int_a^b\norm{\gamma'(t)}\text{d}t$$
moreover $l(\gamma)$ doesn't depend on the parametrization of $\gamma$
\section{Corollary(exercise)}
If $\gamma$ is a curve (piecewise $C^1$), then $\gamma$ is rectifiable and the length is the sum of the length of it's $C^1$ pieces.
\section{Def}
A $C^1$-curve is \textbf{regular} if $\gamma'(t)\neq0$ for any $t\in [a,b]$ A piecewise $C^1$-path (curve) is regular if all its pieces are regular



\section{Def}
$$N:=\frac{T}{\norm{T}}$$ is \textbf{normal vector} of $T$
\section{Def}
Let $\gamma:[a,b]\rightarrow\mathbb{R}^n$ a $C^1$ curve; Let $l$ be the length of $\gamma$ (by theorem proved $l(\gamma)<+\infty$) Let's define the following function:
$$s(t):=\int_a^t\norm{\gamma'(t)}\text{d}u$$
$s(t)$ is the length of $\gamma\mid_{[a,t]}$
THe function $\norm{\gamma'(u)}$ iss continuous, hence
$$s'(t)=\norm{\gamma'(t)}$$
Now assume that $\gamma$ is $C^1$ and \textbf{regucar}($\gamma'(t)\neq0,\forall t\in [a,b]$), then $s'(t)>0$

So $s:[a,b]\rightarrow[0,l]$ is a $C^1$-differmorphism, the inverse is $$t:[0,l]\rightarrow[a,b]$$$$\frac{\text{d}t}{\text{d}s}=frac{1}{\norm{\gamma'(t)}}$$
We reparameterize $\gamma$ with $t$ and get
$$\tilde{\gamma}(s)=(\gamma\circ t)(s)$$
$\tilde{\gamma}:[0,l]\rightarrow\mathbb{R}^n$ we say that $\tilde{\gamma}$ is the reparameterization of $\gamma$ with respect to its \textbf{curvilinear coordinate} $s(t)$
\section{Def}
$f:U\subseteq\mathbb{R}^n\rightarrow\mathbb{R}^n$ $f$ is a $C^{(k)}$-differ if \begin{itemize}
    \item $f$ is of class $C^{(k)}$
    \item $f$ is bijection, and the inverse is $C^{(k)}$
\end{itemize}
\section{Def}
In general $$\gamma:[a,b]\rightarrow\mathbb{R}^n\leadsto \tilde{\gamma}:[0,l]\rightarrow\mathbb{R}^n$$
regular and $C^1$
$$\frac{\text{d}\tilde{\gamma}}{\text{d}s}=\frac{\text{d}\gamma}{\text{d}t}\frac{\text{d}t}{\text{d}s}=\frac{\gamma'(t)}{\norm{\gamma'(t)}}$$
$\norm{\frac{\text{d}\tilde{\gamma}}{\text{d}s}}=1$
$$T(t):=\frac{\text{d}\tilde{\gamma}}{\text{d}s}=\frac{\gamma'(t)}{\norm{\gamma'(t)}}$$
tangent: (vector) $\rightarrow$ vector of norm 1
$$0=\frac{\text{d}}{\text{d}t}\norm{T(t)}^2=\frac{\text{d}}{\text{d}t}\left<T(t),T(t)\right>=2\left<T(t),T'(t)\right>\Leftrightarrow T'(t)\perp T(t)$$
use the fact that in $\mathbb{R}^n$, $u,v:\mathbb{R}\rightarrow\mathbb{R}^n$ differentiable
$$\begin{aligned}
    \frac{\text{d}}{\text{d}t}\left<u(t),v(t)\right> &= \left<\frac{\text{d}u}{\text{d}t},v(t)\right>+\left<u(t),\frac{\text{d}v}{\text{d}t}\right>
\end{aligned}$$
then
$$
\begin{aligned}
    \frac{\text{d}^2\tilde{\gamma}}{\text{d}s^2}&=\frac{\text{d}}{\text{d}s}(\frac{\text{d}\tilde{\gamma}}{\text{d}s})\\
    &=\frac{\text{d}}{\text{d}s}(T(t))\\
    &=\frac{\text{d}T}{\text{d}t}\frac{\text{d}t}{\text{d}s}\\
    &=\frac{T'(t)}{\norm{\gamma'(t)}}    
\end{aligned}$$
$N(t)=\frac{\text{d}^2\tilde{\gamma}}{\text{d}s^2}/\norm{\frac{\text{d}^2\tilde{\gamma}}{\text{d}s^2}}$. If $n=2$
Along the curve we have a 'moving' canonical basis of 
$$\begin{aligned}
    \mathbb{R}^2_{\gamma(t)}&=\span\{T(t),N(t)\}\\
    &=\{\alpha T(t)+\beta N(t)\mid\alpha,\beta\in \mathbb{R}\}
\end{aligned}$$
$\{T(t),N(t)\}$ is a orthonormal basis of $\mathbb{R}^2_{\gamma(t)}$
\section{Def:isometry}
$$(V,g)\stackrel{f}\rightarrow(W,g')$$
a morphism $f$ of vector space with inner product is \textbf{isometry} if $$g(x,y)=g'(f(x),f(y))$$
\section{Def:isometric}
$V\stackrel{\cong}\rightarrow W$ up to isomorphism.

Then $(V,g)$ and $(W,g')$ are \textbf{isometric} if there are two isometry
$$\begin{aligned}
    f:(V,g)&\rightarrow&(W,g')\\
    f':(W,g')&\rightarrow&(V,g)
\end{aligned}$$
such that
$$f\circ f'=f'\circ f=Id$$
\section{Def: Semilinear}
If $V$ and $W$ are two complex vector sapce, then a \textbf{semilinear mapping} is a mapping $f:V\rightarrow W$ such that
\begin{itemize}
    \item $f(v_1+v_2)=f(v_1)+f(v_2)$
    \item $f(\alpha v)=\alpha*f(v)=\overline{\alpha}f(v)$
\end{itemize}
So a semilinear mapping is a linear mapping: $f:V\rightarrow W$

For sesquilinear forms, the theory is similar to the theory of bilinear forms.
$$g\leadsto G(\text{fix a basis})\quad g(x,y)=xG\overline{y}$$
If you change basis, then the Gram matrix changes in the following way:
$$G\leadsto A^TG\overline{A}$$
If $g$ is bilinear
$$g\leadsto \tilde{g}:V\rightarrow V^\vee$$
and $$g\leadsto\tilde{g}:V\rightarrow\overline{V^\vee}$$
linear if $g$ is sesquilinear ($\tilde{g}:V\rightarrow V^\vee$ is semilinear)
\section{Def}
A sesquilinear form $g:V\times \overline{V}\rightarrow K$ is \textbf{hermitian} if
$$g(x,y)=\overline{g(y,x)}$$
And note that inner product is any of symmetric symplectic or hermitian.

\end{document}